## Q. 저자는 왜 로그 방식 부터 언급할까?

**가장 단순한 구조에서 출발해 복잡한 문제(검색 속도)를 해결해 나가는 과정을 보여주기 위함입니다.**

저자가 **로그(Append-only)**부터 설명하는 핵심 의도는 크게 두 가지입니다.

1.  **데이터베이스의 가장 원초적인 트레이드오프(Trade-off) 증명**

    - 로그 방식은 **쓰기 속도가 물리적으로 가장 빠릅니다** (디스크 헤드 이동 최소화).
    - 반면 **읽기 속도는 최악** ($O(n)$)입니다.
    - "쓰기는 빠른데 읽기가 느리네? 그럼 **읽기를 빠르게 하려면 무엇을 추가해야 할까?**"라는 질문을 던지며 자연스럽게 **인덱스(Index)**의 필요성을 도출하기 위함입니다.

2.  **현대 DB의 핵심 구성 요소**
    - 우리가 쓰는 복잡한 DB들도 내부를 뜯어보면 결국 **로그**를 사용합니다.
    - **LSM-Tree** (Cassandra, RocksDB 등)는 이 로그 구조를 정교하게 발전시킨 것이고,
    - **B-Tree** (MySQL, Oracle 등)도 데이터 유실 방지를 위해 **WAL(Write-Ahead Log)**이라는 로그를 가장 먼저 기록합니다.

즉, **"가장 단순하지만 가장 강력한 쓰기 모델"**을 먼저 깔아두고, 그 위에 살을 붙여나가는 방식(로그 → 해시 인덱스 → SSTable → LSM-Tree)으로 설명하기 위해 로그부터 언급한 것입니다.

## Q. 엥 DB들도 내부를 뜯어보면 로그이다 => 이게 무슨소리지

여기서 말하는 **로그(Log)**는 우리가 흔히 보는 `console.log` 같은 텍스트 로그가 아니라, **"순서대로 추가만 되는(Append-only) 데이터 기록물"**이라는 컴퓨터 공학적 의미입니다.

"DB의 본질이 로그다"라는 말은 크게 두 가지 의미로 나뉩니다.

### 1. "살아남기 위해" 로그를 쓴다 (B-Tree 계열)

MySQL, Oracle, PostgreSQL 같은 **B-Tree 기반 DB**들은 데이터를 페이지(Page) 단위로 덮어씁니다. 그런데 덮어쓰는 도중 전원이 꺼지면 데이터가 깨질 수 있습니다.
그래서 이들은 **실제 데이터를 고치기 전에, "나 이거 고칠 거야"라고 별도의 파일에 줄글로 먼저 적습니다.**
이것이 바로 **WAL (Write-Ahead Log, 미리 쓰는 로그)**입니다.

- **동작:** `WAL에 기록(Append)` -> `성공하면 메모리/디스크의 B-Tree 수정`
- **복구:** DB가 뻗었다 켜지면, **WAL(로그)을 처음부터 다시 읽어서** 깨진 B-Tree를 복구합니다.
- **결론:** B-Tree가 화려한 건물이라면, **로그(WAL)는 그 건물의 설계도이자 원본**입니다.

### 2. "구조 자체가" 로그다 (LSM-Tree 계열)

Cassandra, RocksDB 같은 **LSM-Tree 기반 DB**들은 아예 데이터 저장 방식 자체가 로그입니다.
데이터를 수정할 때 기존 위치를 찾아가서 고치지 않고, **새로운 값을 그냥 맨 뒤에 추가(Append)**합니다.

- `x = 1` 저장 (추가)
- `x = 2` 수정 요청 -> 그냥 `x = 2`를 맨 뒤에 또 추가
- 읽을 때는 **"로그의 맨 뒤(최신)"**에 있는 `x = 2`를 가져감.
- **결론:** 얘네는 그냥 **거대한 로그 파일들의 집합** 그 자체입니다.

### 요약

따라서 **"DB를 뜯어보면 로그다"**라는 말은 과장이 아닙니다.

- **B-Tree DB**: 로그(WAL)가 없으면 **데이터 복구**를 못 해서 망함.
- **LSM-Tree DB**: 구조 자체가 로그라서 로그 없으면 **성립**이 안 됨.

결국 데이터베이스에서 **가장 진실된 데이터(Source of Truth)**는 복잡한 자료구조가 아니라, 차곡차곡 쌓인 **로그** 파일입니다.

## Q. WAL이 뭐지..?

- MySQL, Oracle, PostgreSQL 같은 B-Tree 기반 DB들은 데이터를 페이지(Page) 단위로 덮어씁니다. 그런데 덮어쓰는 도중 전원이 꺼지면 데이터가 깨질 수 있습니다.
- 그래서 이들은 실제 데이터를 고치기 전에, "나 이거 고칠 거야"라고 별도의 파일에 줄글로 먼저 적습니다.

- WAL은 순산적으로 메모리에 있다가 디스크에 쓰여짐
- 이는 디스크의 데이터가 바뀔 때 함께 작동함.
- 마치 아웃박스 패턴과 유사한데, 사용처는 많이 다름 (이벤트 전파 vs 내결함성)
- WAL에 쓰는 시간 좀 더 빨라서, 복구용으로 씀. 그리고 WAL에 적용되면 먼저 완료되었다고 날리는 방식도 있다고 하던데 정확히는 이해못함.
- 아 그렇다면 이제 이해가 됨. B-Tree와 LSM-Tree 모두 거의 일급 객체로 로그를 취급한다는 사실.

## Q. mongodb 같은 document db도 append only야 내부적으로?

기본적으로는 **아닙니다.**

MongoDB(기본 엔진인 **WiredTiger**)는 **B-Tree** 구조를 사용합니다. 즉, LSM-Tree처럼 무조건 끝에 붙이는 게 아니라, 데이터가 들어갈 위치(페이지)를 찾아서 그 부분을 **수정(Update-in-place/Overwrite)**하는 방식이 메인입니다.

다만, 내부를 자세히 들여다보면 **Append-only** 요소가 없지는 않습니다.

1.  **저널 (Journaling = WAL)**
    - MySQL의 WAL처럼, 실제 B-Tree를 건드리기 전에 "나 이거 고칠 거야"라고 기록하는 **복구용 로그**는 **Append-only**입니다.
2.  **Oplog (Operations Log)**
    - 복제(Replication)를 위해 사용하는 로그도 순차적으로 쌓이는 **Append-only**입니다. (이걸 보고 Slave 노드들이 데이터를 따라갑니다.)

**요약하자면:**
데이터 파일 자체는 **덮어쓰기(B-Tree)** 방식이지만,
데이터의 안전(복구)과 복제를 위해 **로그(Append-only)**를 보조적으로 아주 중요하게 사용합니다.

## Q. b-tree에서 update는 insert보다 오버헤드가 적어?

일반적으로는 **Update가 Insert보다 오버헤드가 적거나 비슷합니다.**

하지만 **"데이터의 크기가 변하느냐"**에 따라 상황이 완전히 달라집니다. 핵심은 **페이지 분할(Page Split)** 발생 여부입니다.

### 1. Update가 더 빠른 경우 (대부분의 상황)

데이터의 크기가 그대로이거나 작아지는 경우입니다.

- **동작:** 해당 키가 있는 페이지를 찾아서, 그 공간의 비트만 쓱 바꿉니다.
- **비용:** 트리의 구조를 건드릴 필요가 없습니다. (균형 맞추기, 페이지 쪼개기 등 불필요)
- **결과:** **Update 승**

### 2. Update가 Insert만큼 느려지는 경우 (최악의 상황)

데이터 내용을 수정했는데, **수정된 데이터가 기존보다 훨씬 커져서 페이지 용량을 초과**하는 경우입니다.

- **동작:** 4KB 페이지에 빈 공간이 없다면, 결국 **페이지 분할(Page Split)**이 일어납니다.
- **비용:** 새 페이지 할당 + 데이터 이동 + 부모 노드 포인터 갱신 등 **Insert가 페이지를 꽉 채웠을 때 하는 무거운 작업을 똑같이 수행**해야 합니다.
- **결과:** **Insert와 동급**

### 3. Insert는 왜 기본적으로 비용이 더 들까?

Insert는 필연적으로 **"새로운 공간"**을 잡아먹습니다.

- 운이 좋아서 페이지에 빈 공간이 있으면 그냥 넣으면 되지만(Update와 비슷),
- 데이터가 계속 쌓이다 보면 **반드시 페이지 분할(Split)이 발생하는 시점**이 옵니다. 이 확률 때문에 평균 비용은 Insert가 더 높게 잡힙니다.

---

### ⚠️ 주의: 인덱스 컬럼을 Update 할 때

만약 `UPDATE users SET name = 'NewName' ...`을 하는데, `name` 컬럼에 인덱스가 걸려 있다면 이야기가 다릅니다.
이때는 단순 덮어쓰기가 아니라, 내부적으로 **Delete + Insert**가 일어납니다.

1.  기존 인덱스 엔트리 삭제 (Delete)
2.  새로운 인덱스 엔트리 삽입 (Insert) -> 여기서 페이지 분할 가능성 있음

이 경우 Update가 Insert보다 오히려 더 비쌀 수도 있습니다.

만약 누군가 "MongoDB는 LSM-Tree처럼 Append-only 방식이라서 쓰기가 빨라"라고 했다면 그건 틀린 설명입니다. 그건 Cassandra나 HBase 같은 DB에 해당하는 말입니다.

## Q. 흠.. 그러면 mongodb는 처음에는 lsm tree를 지향하다가 어느순간부터 사실상 b tree 가족에 편입된건가 왜그랬찌

사실 약간의 오해가 있습니다! MongoDB는 **"LSM에서 B-Tree로 전향한 것"이 아닙니다.**

역사를 보면 MongoDB는 태생부터 **"Update-in-place (덮어쓰기)"** 진영이었습니다.

### 1. MongoDB의 엔진 역사

1.  **초창기 (MMAPv1)**:

    - 운영체제의 `mmap()` 시스템 콜을 이용해 파일을 메모리에 1:1로 매핑해서 쓰는 아주 단순 무식한 방식이었습니다.
    - 이때도 구조는 **B-Tree**였습니다. 다만, 파일에 빈 공간(Padding)을 미리 만들어두고, 데이터가 커지면 그 공간을 쓰는 식의 **"제자리 수정"** 방식이었습니다.
    - **문제점:** 데이터가 커져서 빈 공간을 넘치면 데이터를 통째로 이사시켜야 했고, **락(Lock)을 DB 전체 단위**로 걸어서 동시성 성능이 최악이었습니다.

2.  **현재 (WiredTiger, 3.2 버전부터 기본)**:
    - MongoDB가 2014년에 WiredTiger라는 회사를 인수하면서 엔진을 갈아끼웠습니다.
    - WiredTiger는 **B-Tree**를 아주 정교하게 구현한 엔진입니다.
    - **개선점:** 이제는 **행(Document) 단위로 락**을 걸 수 있게 되었고, 압축 기능이 추가되었습니다.

---

### 2. 왜 (LSM이 아닌) B-Tree를 선택했을까?

WiredTiger 엔진 자체는 사실 **LSM-Tree 모드도 지원**합니다. 하지만 MongoDB는 이를 **B-Tree 모드**로 사용하도록 설정해 두었습니다. 그 이유는 **MongoDB의 정체성** 때문입니다.

1.  **읽기 성능의 예측 가능성 (Predictable Read Performance)**

    - **LSM-Tree**: 읽을 때 Memtable → 최신 SSTable → 옛날 SSTable 순으로 뒤져야 하므로, 상황에 따라 읽기 속도 편차가 큽니다.
    - **B-Tree**: 데이터가 어디 있든 트리의 높이(Depth)만큼만 가면 되므로 읽기 속도가 **일정하고 빠릅니다.**
    - MongoDB는 "범용 데이터베이스"를 지향하므로, 쓰기 몰빵형(LSM)보다는 읽기/쓰기 밸런스가 좋은 B-Tree가 맞습니다.

2.  **풍부한 인덱스 지원**
    - MongoDB는 RDBMS처럼 다양한 **보조 인덱스(Secondary Index)**를 지원합니다.
    - LSM-Tree 구조에서 보조 인덱스를 관리하는 것은 B-Tree보다 훨씬 복잡하고 비용이 듭니다. (데이터 일관성 유지 등)

### 요약

MongoDB는 처음부터 끝까지 **"덮어쓰기(Mutable) & B-Tree"** 파벌이었습니다. 다만 초기의 **허접한 B-Tree(MMAPv1)**에서 **세련된 B-Tree(WiredTiger)**로 진화했을 뿐입니다.

## Q. ⭐ 메모리 저장에서 - 위치 정보도 디스크에 넣으면 무한대 아닌가? 그러면 io 두번 거쳐서 느리다는건가

네, **정확한 통찰입니다.**

사용자님의 말씀대로 인덱스(위치 정보)를 디스크에 저장하면 메모리 용량의 제약은 사라지지만, **성능이 급격히 떨어지는 문제**가 발생합니다.

### 1. 왜 느려지는가? (Random I/O 문제)

말씀하신 대로 최소 **두 번의 디스크 접근**이 필요해집니다.

1.  **첫 번째 I/O**: 디스크에 있는 인덱스 파일에서 "키가 어디 있는지" 찾기 위해 읽어야 함.
2.  **두 번째 I/O**: 알아낸 위치로 이동해서 실제 데이터를 읽어야 함.

문제는 하드디스크(HDD)나 SSD에서 임의의 위치를 찾아가는 **랜덤 I/O(Seek)**가 가장 느린 작업이라는 점입니다. Bitcask는 "단 한 번의 랜덤 I/O"를 보장하기 위해 인덱스를 무조건 메모리에 올리는 극단적인 설계를 택한 것입니다. 인덱스가 디스크에 내려가는 순간, Bitcask만의 장점(압도적인 읽기 속도)이 사라집니다.

### 2. 그래서 나온 해결책들 (요약 문서 연결)

메모리 한계는 극복하면서 성능도 잡기 위해 등장한 것이 문서에 있는 **SSTable**과 **B-Tree**입니다.

- **SSTable (LSM-Tree)의 접근법**:

  - "모든 키를 메모리에 올리는 건 무리지만, **일부(Sparse Index)**만 올리는 건 괜찮지 않을까?"
  - 데이터를 **정렬**해두면, 메모리에는 '가', '나', '다'의 시작 위치만 가지고 있어도 그 사이 데이터를 찾을 수 있습니다. 이렇게 하면 **적은 메모리로도 무한대에 가까운 데이터를 빠르게** 찾을 수 있게 됩니다. (문서 1.3절)

- **B-Tree의 접근법**:
  - "아예 인덱스를 **디스크에 저장하는 것을 전제로 최적화**하자."
  - 인덱스를 **페이지(Page)** 단위로 쪼개고 트리 구조로 만들어서, 디스크에 있더라도 적은 횟수의 이동(Depth)만으로 키를 찾을 수 있게 설계했습니다. (문서 1.4절)

결론적으로 **Bitcask(해시 인덱스)**는 "데이터셋이 작아서 키가 메모리에 다 들어갈 때"만 쓸 수 있는 특수 목적 엔진이고, 범용적인 대용량 처리를 위해서는 **SSTable**이나 **B-Tree** 같은 구조를 사용합니다.

# Q. lsm tree와 b tree의 관계가 상당히 흥미로운게 - 가상 메모리 페이징이랑 세그멘테이션이랑 엄청 비슷하네. OS는 사실상 페이징이 다 이기고 있는 걸로 아는데, OS도 세그멘테이션으로 구현한 케이스가 있나 상용중에

[4 tools called]

사용자의 통찰력이 매우 흥미롭습니다. 말씀하신 대로 **LSM Tree (가변 크기, 순차 쓰기)와 B-Tree (고정 크기, 랜덤 쓰기)**의 관계는 **세그멘테이션(가변 크기, 논리적 단위)과 페이징(고정 크기, 물리적 단위)**의 관계와 구조적으로 매우 닮아 있습니다.

질문하신 **"OS도 세그멘테이션으로 구현한 상용 케이스가 있는가?"**에 대한 답은 **"네, 존재했고 지금도 특정 분야에서 살아있습니다. 하지만 범용 시장에서는 페이징이 압승했습니다."**로 요약할 수 있습니다.

상용 OS 중 세그멘테이션을 핵심으로 사용한 대표적인 케이스들을 정리해 드립니다.

### 1. 세그멘테이션의 끝판왕: Burroughs MCP (현 Unisys ClearPath)

가장 유명한 "세그멘테이션 승리" 사례입니다.

- **특징:** 1961년에 나온 Burroughs 대형 컴퓨터 시스템(B5000 등)은 하드웨어 자체가 고수준 언어(ALGOL)를 위해 설계되었습니다.
- **구현:** 아예 하드웨어 레벨에서 **페이징 없이 순수 세그멘테이션**만 사용했습니다. 모든 메모리 접근이 '디스크립터(Descriptor)'라는 것을 통해 이루어졌고, 배열의 경계 검사 등을 하드웨어가 자동으로 수행했습니다.
- **현재:** 이 계보는 현재 **Unisys의 ClearPath Libra** 시스템으로 이어져 금융권 등 미션 크리티컬한 분야에서 여전히 현역으로 뛰고 있습니다.

### 2. 하이브리드(세그멘테이션 + 페이징): Multics와 IBM OS/2

- **Multics:** 현대 OS의 조상 격인 Multics는 **'페이징된 세그멘테이션(Paged Segmentation)'**을 사용했습니다. 프로그래머에게는 세그먼트(논리적 뷰)를 보여주고, 실제 물리 메모리는 페이지(물리적 효율)로 관리하는 방식입니다.
- **IBM OS/2 & Windows 3.x:** Intel 80286(16비트 보호 모드) 시절은 하드웨어 자체가 세그멘테이션 기반이었습니다. 당시 Windows 3.0/3.1이나 초기 OS/2는 세그먼트 단위(64KB 제한 등)로 메모리를 관리해야 했습니다. (이후 386부터 페이징이 도입되면서 'Flat Memory Model'로 넘어가게 됩니다.)

### 3. 독특한 철학: IBM AS/400 (현 IBM i)

- **Single Level Store:** 이 시스템은 메인 메모리(RAM)와 디스크를 구분하지 않고 **하나의 거대한 64비트(현재 128비트) 주소 공간**으로 봅니다.
- **객체 기반:** 모든 것이 '객체'로 관리되며, 이 객체들은 본질적으로 가변 크기의 세그먼트입니다. 하드웨어와 OS가 긴밀하게 결합되어 있어, 파일 입출력 개념 대신 메모리 주소 접근 방식을 사용합니다.

### 요약 및 사용자의 통찰 연결

사용자님의 비유를 연결해보면 역사가 더 재미있게 보입니다.

- **Paging (B-Tree)의 승리 이유:**

  - **관리의 단순함:** 가변 크기 덩어리(세그먼트)를 관리하면 '외부 단편화(External Fragmentation)'라는 골치 아픈 빈 공간 맞추기 테트리스를 해야 합니다. (마치 힙 메모리 관리처럼요).
  - **하드웨어 효율:** 4KB 같은 고정 크기로 자르면 하드웨어 구현이 훨씬 쉽고 빠릅니다. B-Tree가 디스크 블록(Page) 친화적인 것과 똑같습니다.

- **LSM Tree (Segmentation?)의 부상:**
  - 하지만 최근 스토리지(SSD)와 워크로드(대량 쓰기)의 변화로 LSM Tree가 뜨는 것처럼, 클라우드 네이티브나 언어 런타임 레벨(Java/Go 힙, WASM 등)에서는 다시 **"가변 크기의 논리적 덩어리"**를 다루는 개념들이 중요해지고 있습니다.

결론적으로 **범용 OS 시장(x86/ARM + Linux/Windows)은 페이징이 통일**했지만, **금융/메인프레임(Unisys, IBM i) 같은 특수 목적 고성능 시장**에서는 세그멘테이션 철학이 여전히 강력하게 살아있습니다.

# Q. `하지만 최근 스토리지(SSD)와 워크로드(대량 쓰기)의 변화로 LSM Tree가 뜨는 것처럼, 클라우드 네이티브나 언어 런타임 레벨(Java/Go 힙, WASM 등)에서는 다시 "가변 크기의 논리적 덩어리"를 다루는 개념들이 중요해지고 있습니다` => 이거에 대해 자세히 알려줘. 대량 쓰기에 LSM Tree가 유리한 이유는 insert를 말하는거야? 아니면 update?

사용자님의 질문은 현대 컴퓨팅 시스템의 **"추상화 계층(Software)과 물리 계층(Hardware)의 간극을 어떻게 메우는가"**에 대한 핵심을 찌르는 질문입니다.

두 가지 질문에 대해 순서대로, 명쾌하게 답변해 드리겠습니다.

---

### 1. LSM Tree가 "대량 쓰기"에 유리한 이유 (Insert vs Update?)

결론부터 말씀드리면 **"Insert와 Update (심지어 Delete까지) 모두"**입니다. 왜냐하면 LSM Tree의 세계에서 이 셋은 **물리적으로 완전히 동일한 작업**이기 때문입니다.

#### 1) "덮어쓰기(In-place)" vs "추가하기(Append-only)"

- **B-Tree (기존 DB):** `UPDATE user SET age=30 WHERE id=1`을 수행하면, B-Tree는 디스크에서 `id=1`이 저장된 **위치를 찾아가서 그 부분만 지우고 다시 씁니다.** (Random Write).
- **LSM Tree:** `id=1`의 위치를 찾지 않습니다. 그냥 맨 뒤에 `[key:1, value:30]`이라는 **새로운 로그를 한 줄 추가**하고 끝냅니다. (Sequential Write).

#### 2) 왜 유리한가?

LSM Tree에서는 **Insert, Update, Delete가 전부 "새로운 데이터 추가(Append)"**로 처리됩니다.

- **Update:** "최신 버전의 로그"를 추가함. (나중에 읽을 때 최신 것만 읽으면 됨)
- **Delete:** "이 데이터는 삭제됨(Tombstone)"이라는 표식(마커)을 추가함.
- **Insert:** 그냥 추가함.

따라서 기존 데이터가 어디 있는지 찾을 필요(Seek)가 없고, 디스크 헤더를 움직일 필요 없이 **무조건 끝에다가 들이붓기만 하면 되기 때문에** 쓰기 성능이 압도적입니다.

---

### 2. "가변 크기의 논리적 덩어리"가 다시 중요해진 이유 (SSD & 언어 런타임)

사용자님이 말씀하신 대로 OS 레벨에서는 '고정 크기 페이징'이 이겼지만, **그 위(애플리케이션/런타임)와 그 아래(SSD 내부)**에서는 '가변 크기 또는 로그 구조'가 다시 각광받고 있습니다. 이를 "세그멘테이션의 귀환"이라고 볼 수 있는 흥미로운 이유들이 있습니다.

#### ① SSD의 물리적 특성: "플래시 메모리는 덮어쓰기를 싫어한다"

하드디스크(HDD)와 달리 SSD는 **'덮어쓰기(Overwrite)'가 불가능**합니다.

- 데이터를 수정하려면 **지우고(Erase) -> 다시 써야(Write)** 하는데, 지우는 단위(Block, 보통 수 MB)가 쓰는 단위(Page, 보통 4KB)보다 훨씬 큽니다.
- B-Tree처럼 4KB짜리 페이지 하나를 수정하려고 하면, SSD 내부에서는 **[거대한 블록 읽기 -> 4KB 수정 -> 거대한 블록 다시 쓰기]**라는 비효율(Write Amplification)이 발생합니다.
- **LSM Tree의 해법:** 데이터를 수정하지 않고, **거대한 덩어리(SSTable)를 만들어 순차적으로 씁니다.** 이는 SSD가 가장 좋아하는 패턴이며, SSD의 수명과 성능을 극대화합니다.

#### ② 언어 런타임(Java/Go)의 힙 메모리 관리: "소프트웨어적 세그멘테이션"

Java나 Go 같은 매니지드 언어의 힙(Heap) 메모리 관리는 **LSM Tree와 놀라울 정도로 비슷**합니다.

- **Bump Allocation:** 객체를 생성할 때(New), 빈 공간을 찾아 헤매지 않고 **포인터만 쭉쭉 이동**하며 순차적으로 할당합니다. (LSM의 MemTable에 Append하는 것과 동일)
- **Compacting GC (가비지 컬렉션):** 힙에 쓰레기(삭제된 객체)가 쌓이면, 살아남은 객체들만 모아서 **새로운 영역으로 예쁘게 복사(Copy)**하고 기존 영역을 통째로 비웁니다.
  - 이 과정은 **LSM Tree의 Compaction(SSTable 병합)**과 개념적으로 **완벽하게 동일**합니다.
  - 즉, OS는 페이징을 주더라도, **언어 런타임은 그 위에서 가변 크기의 객체들을 '논리적 덩어리(Generation/Region)' 단위로 관리**하는 것이 효율적이라는 것을 깨달은 것입니다.

#### ③ 클라우드 네이티브 & WASM

- **Immutable Infrastructure:** 도커 컨테이너나 서버리스 함수는 실행 중에 코드가 바뀌지 않습니다. 배포할 때마다 새로운 이미지(Layer)가 통째로 올라갑니다. 이는 **"수정"이 아니라 "새로운 버전의 스냅샷(Segment) 추가"**라는 개념입니다.
- **WASM (WebAssembly):** WASM은 보안을 위해 '선형 메모리(Linear Memory)'라는 하나의 거대한 배열 공간을 할당받아 씁니다. 이 공간 안에서 어떻게 데이터를 쪼개 쓸지는 WASM 모듈이 알아서 합니다. OS 입장에서는 이것이 하나의 거대한 **세그먼트**처럼 보입니다.

### 요약

1.  **LSM Tree의 대량 쓰기:** `Insert`, `Update`, `Delete`를 모두 **"순차적 추가(Append)"**로 변환하기 때문에 빠릅니다. (물리적 수정 비용 0)
2.  **가변 덩어리의 귀환:**
    - **하드웨어(SSD):** 작은 수정보다 큰 덩어리 쓰기를 선호함.
    - **소프트웨어(GC, Cloud):** 복잡한 관리 비용을 줄이기 위해, 자잘한 관리는 런타임에 맡기고 **"쓰기는 순차적으로, 정리는 나중에 한꺼번에(Compaction)"** 하는 철학이 대세가 되었기 때문입니다.

결국 **"복잡도 보존의 법칙"**처럼, OS가 페이징으로 하드웨어 관리를 단순화시키니, 그 위의 애플리케이션(DB, 런타임) 레벨에서 다시 논리적인 효율성을 위해 세그멘테이션(LSM, GC Region) 같은 기법을 구현하고 있는 셈입니다.

# Q. ⭐ 그럼 SSD랑 b-tree db는 엄청 호환이 안좋아?

결론부터 말씀드리면, **"호환이 안 좋은 게 아니라, 오히려 SSD가 B-Tree 데이터베이스를 멸종 위기에서 구했다"**고 보는 것이 더 정확합니다.

다만, 사용자님의 통찰대로 **"기계적인(물리적인) 찰떡궁합"은 LSM Tree가 SSD와 훨씬 더 잘 맞는 것이 사실**입니다.

이 미묘한 **"B-Tree와 SSD의 애증 관계"**를 3가지 포인트로 정리해 드릴게요.

---

### 1. 왜 "구원자"인가? (Random Read의 혁명)

B-Tree의 구조를 생각해보세요. 루트에서 리프 노드까지 포인터를 따라 **점프(Seek)**해야 합니다.

- **HDD 시절:** 디스크 헤드가 물리적으로 이동해야 하므로, B-Tree의 깊이가 깊어지거나 데이터가 파편화되면 성능이 끔찍하게 느려졌습니다. (Random I/O 최악)
- **SSD 등장:** SSD는 헤드가 없습니다. 전기 신호로 접근하므로 **Random Read 속도가 HDD보다 수백 배 빠릅니다.**
- **결과:** 오라클, MySQL 같은 전통적인 B-Tree 기반 DB들이 SSD를 만나면서 병목이 사라지고 성능이 비약적으로 상승했습니다. **"B-Tree는 SSD 덕분에 날개를 달았다"**는 표현이 맞습니다.

### 2. 어디가 "호환이 안 좋은"가? (Write Amplification)

사용자님이 정확하게 보신 부분입니다. **쓰기(Write)** 측면에서는 구조적인 비효율이 존재합니다.

- **B-Tree의 방식 (In-place Update):**
  - 데이터 10바이트를 고치려 해도, DB는 **페이지 단위(보통 4KB~16KB)**로 덮어써야 합니다.
- **SSD의 고통 (Write Amplification):**
  - SSD는 덮어쓰기가 안 되므로, **[블록 읽기 -> 수정 -> 빈 블록에 쓰기 -> 기존 블록 마킹]** 과정을 거칩니다.
  - 즉, **"고작 10바이트 고치려고 SSD 수명을 깎아먹고 내부적으로 엄청난 복사 작업을 유발"**하는 현상이 발생합니다.

이 점 때문에 **"쓰기 헤비(Write-heavy)한 워크로드"**에서는 B-Tree가 SSD의 잠재력을 100% 못 끌어내고 수명도 빨리 닳게 합니다. 반면 LSM Tree는 이걸 순차 쓰기로 바꿔주니 SSD와 천생연분인 것이죠.

### 3. 그런데 왜 아직도 잘 쓰는가? (FTL과 버퍼의 마법)

그럼에도 불구하고 MySQL(InnoDB)이나 PostgreSQL이 SSD에서 펄펄 나는 이유는, 중간에서 **완충 장치**들이 엄청난 일을 해주기 때문입니다.

1.  **DB 내부의 버퍼 (Buffer Pool):**
    - DB는 데이터가 바뀌었다고 바로 디스크에 쓰지 않습니다. 메모리(Buffer)에 모아뒀다가(Dirty Page), 체크포인트(Checkpoint) 때 **모아서 한방에** 씁니다. 이게 랜덤 쓰기를 어느 정도 순차 쓰기처럼 만들어줍니다.
2.  **SSD 컨트롤러의 사기극 (FTL - Flash Translation Layer):**
    - SSD 컨트롤러(펌웨어)는 OS가 "랜덤하게 여기저기 써라"라고 명령해도, 내부적으로는 **LSM Tree처럼 빈 공간에 순차적으로 차곡차곡 채워 넣습니다.**
    - 즉, **SSD 하드웨어 자체가 이미 내부적으로 로그 구조(Log-Structured) 방식으로 동작**하며 B-Tree의 비효율을 하드웨어 레벨에서 커버하고 있는 것입니다.

### 요약

- **읽기(Read):** SSD와 B-Tree는 **최고의 파트너**입니다. (랜덤 탐색 비용 0)
- **쓰기(Write):** 구조적으로는 **상극**이 맞습니다. (쓰기 증폭 발생)
- **현실:** DB 엔진의 최적화와 SSD 컨트롤러(FTL)의 기술력으로 그 상극 관계를 극복하고 잘 쓰고 있습니다.

하지만 **"극한의 쓰기 성능"**이 필요한 시스템(예: 로그 수집, 거래소 체결 시스템)에서는 이러한 FTL의 오버헤드조차 아깝기 때문에, 하드웨어 특성을 그대로 따라가는 **LSM Tree (RocksDB, Cassandra 등)**를 선택하는 것입니다.

## Q. 랜덤 탐색이라는건 뭐지?

**"랜덤 탐색(Random Access/Seek)"**은 우리가 책을 읽을 때 **"목차를 보고 230페이지를 바로 펼치는 것"**과 같습니다. 반대말은 **"순차 탐색(Sequential Access)"**으로, 1페이지부터 230페이지까지 **한 장 한 장 넘기며 읽는 것**입니다.

컴퓨터, 특히 저장 장치(디스크)에서의 랜덤 탐색이 무엇인지 비유와 실제 작동 원리로 설명해 드릴게요.

### 1. 비유: 도서관 사서의 움직임

- **순차 탐색 (Sequential):**

  - 사서가 책장에 꽂힌 책을 **왼쪽 끝부터 오른쪽 끝까지** 순서대로 쭉 훑으며 읽습니다.
  - **특징:** 고개를 돌리거나 발을 움직일 필요 없이 시선만 이동하면 되므로 **매우 빠릅니다.**

- **랜덤 탐색 (Random):**
  - 사용자가 "A열 3번 책", "Z열 90번 책", "B열 12번 책"을 가져오라고 시킵니다.
  - 사서는 **A열로 뛰어갔다가 -> Z열로 뛰어갔다가 -> 다시 B열로 뛰어가야** 합니다.
  - **특징:** 책을 읽는 시간보다 **'이동하는 시간(Seek Time)'**이 훨씬 더 많이 걸립니다.

### 2. 하드디스크(HDD)에서의 랜덤 탐색 (물리적 이동)

B-Tree에서 데이터를 찾을 때 랜덤 탐색이 왜 느린지 HDD 구조를 보면 바로 이해됩니다.

- **플래터(원판):** 데이터가 저장된 판이 빙글빙글 돕니다 (LP판처럼).
- **헤드(바늘):** 데이터를 읽는 바늘이 있습니다.
- **랜덤 탐색 과정:**
  1.  데이터가 있는 트랙(바깥쪽/안쪽)으로 헤드를 물리적으로 이동시킵니다. (**Seek Time**)
  2.  원판이 돌아서 데이터가 내 발밑에 올 때까지 기다립니다. (**Rotational Latency**)
- **문제점:** 이 물리적인 이동 시간이 컴퓨터 입장에서는 '영겁의 시간'입니다. 그래서 HDD 시절에는 랜덤 탐색을 줄이는 게 성능 튜닝의 제1원칙이었습니다.

### 3. SSD에서의 랜덤 탐색 (논리적 점프)

SSD는 물리적으로 움직이는 부품이 없습니다. 그런데 왜 랜덤 탐색이라는 용어를 쓸까요?

- SSD는 메모리 주소(번지수)만 알면 전기 신호를 보내 **즉시 접근**할 수 있습니다.
- HDD처럼 물리적으로 이동하는 시간은 없지만, **"연속되지 않은 주소를 띄엄띄엄 요청하는 행위"** 자체를 여전히 랜덤 탐색이라고 부릅니다.
- 하지만 물리적 제약이 없기 때문에 HDD보다 수백 배 빠릅니다. 그래서 **"B-Tree(랜덤 탐색 위주)가 SSD를 만나 날개를 달았다"**고 하는 것입니다.

### 요약

**"랜덤 탐색"**이란 데이터가 저장된 위치가 **여기저기 흩어져 있을 때**, 그 위치를 찾아가기 위해 **건너뛰는(Jump) 행위**를 말합니다.

- **B-Tree:** 구조상 데이터를 찾으러 여기저기 점프해야 함 -> **랜덤 탐색 위주**
- **LSM Tree:** 데이터를 무조건 차곡차곡 쌓고, 읽을 때도 덩어리째 읽음 -> **순차 탐색 위주**

HDD에서는 이 차이가 성능을 갈랐지만, SSD에서는 랜덤 탐색도 엄청나게 빨라져서 그 격차가 많이 줄어든 것입니다.
