## 개요

### 1. 분산 데이터베이스의 데이터 분산 방식

데이터를 여러 노드에 분산하는 방법은 크게 두 가지가 있습니다.

1.  **복제 (Replication)**
    - 같은 데이터의 복사본을 여러 노드에 저장하는 방식입니다.
    - (6장에서 다룬 내용)
2.  **샤딩 (Sharding) / 파티셔닝 (Partitioning)**
    - 모든 데이터를 모든 노드에 저장하지 않고, **큰 데이터를 작은 조각(Shard/Partition)으로 쪼개어 서로 다른 노드에 저장**하는 방식입니다.
    - (7장의 핵심 주제)

### 2. 샤딩(Sharding)의 핵심 개념

- **데이터의 소유권:** 일반적으로 각 데이터 조각(레코드, 로우, 문서)은 **정확히 하나의 샤드**에만 속하도록 정의됩니다.
- **독립성:** 각 샤드는 그 자체로 하나의 작은 데이터베이스 역할을 합니다. (물론 여러 샤드에 걸친 연산을 지원하는 시스템도 존재합니다.)
- **목적:** 데이터셋이 너무 커서 한 대의 머신에 저장하거나 처리할 수 없을 때, 이를 수평적으로 확장(Scale-out)하기 위함입니다.

### 3. 샤딩과 복제의 결합 (Architecture)

샤딩은 보통 복제와 함께 사용됩니다.

- **이유:** 내결함성(Fault Tolerance)을 확보하기 위함입니다.
- **구조:**
  - 각 레코드는 하나의 샤드에 속하지만, 그 **샤드 자체는 여러 노드에 복제**되어 저장됩니다.
  - **리더-팔로워 모델 예시 (Figure 7-1):**
    - 한 노드는 특정 샤드의 **리더(Leader)** 역할을 하면서, 동시에 다른 샤드의 **팔로워(Follower)** 역할을 할 수 있습니다.
    - 예: 노드 A는 '샤드 1'의 리더이면서 '샤드 2'의 팔로워일 수 있음.
  - 이 장에서는 설명의 단순화를 위해 복제 이슈는 배제하고 샤딩 기법 자체에 집중합니다.

### 4. 다양한 용어 정리 (Terminology)

'샤드(Shard)'라는 개념은 사용하는 소프트웨어에 따라 매우 다양한 이름으로 불립니다.

| 용어 (Term)     | 사용하는 시스템 예시           |
| :-------------- | :----------------------------- |
| **Partition**   | Kafka                          |
| **Range**       | CockroachDB                    |
| **Region**      | HBase, TiDB                    |
| **Tablet**      | Bigtable, YugabyteDB, ScyllaDB |
| **Vnode**       | Riak                           |
| **Token-range** | Cassandra                      |
| **vBucket**     | Couchbase                      |

### 5. 샤딩 vs 파티셔닝 (용어의 미묘한 차이)

대부분의 시스템에서는 '파티셔닝'과 '샤딩'을 동의어로 사용하지만, 일부(예: PostgreSQL)에서는 구분하기도 합니다.

- **PostgreSQL의 구분:**
  - **Partitioning:** 큰 테이블을 여러 파일로 쪼개어 **같은 머신**에 저장하는 것 (예: 특정 파티션을 통째로 삭제하는 속도 향상 등의 이점).
  - **Sharding:** 데이터셋을 쪼개어 **여러 머신**에 분산 저장하는 것.
- **일반적인 맥락:** DDIA 책에서는 '파티셔닝'이라는 용어를 '샤딩'과 같은 의미로 사용합니다.

### 6. 어원 및 주의사항

- **샤딩(Sharding)의 유래설:**
  1.  **울티마 온라인(Ultima Online):** 게임 내에서 마법 수정이 산산조각(shattered) 나면서, 각 조각(shard)이 게임 세계의 복사본을 굴절시켜 보여주었다는 설정에서 유래했다는 설. (병렬 게임 서버를 의미하게 됨)
  2.  **약어설:** 1980년대 데이터베이스 시스템인 'System for Highly Available Replicated Data'의 약자라는 설.
- **주의사항 (Disambiguation):**
  - 여기서 말하는 **파티셔닝(Partitioning)**은 **네트워크 파티션(Network Partition, Netsplit)**과는 전혀 다른 개념입니다.
  - 네트워크 파티션은 노드 간 통신이 끊기는 '네트워크 결함'을 의미하며, 이는 9장에서 다룹니다.

## 1. 샤딩의 장단점 (Pros and Cons of Sharding)

샤딩은 데이터베이스 확장성을 위한 강력한 도구이지만, 복잡성을 수반하므로 신중하게 도입해야 합니다.

#### 1.1. 샤딩을 하는 주된 이유: 확장성 (Scalability)

- **목적:** 데이터의 양(Volume)이나 **쓰기 처리량(Write Throughput)**이 단일 노드의 한계를 넘어설 때 사용합니다.
  - _참고:_ 단순히 읽기 처리량(Read Throughput)만 문제라면, 샤딩보다는 6장에서 다룬 '읽기 확장(Read Scaling/Replication)'이 더 적합할 수 있습니다.
- **수평적 확장 (Horizontal Scaling):** 더 큰 머신으로 교체하는 것이 아니라, 더 많은(작은) 머신을 추가하여 용량을 늘리는 방식입니다.
- **병렬 처리:** 워크로드를 균등하게 나누면(각 샤드가 비슷한 부하를 담당), 여러 머신에서 데이터와 쿼리를 병렬로 처리할 수 있습니다.

#### 1.2. 샤딩의 단점과 복잡성

샤딩은 대규모 시스템에는 필수적이지만, 소규모 시스템에서는 오버헤드가 될 수 있습니다. 단일 머신으로 처리가 가능하다면 샤딩을 피하는 것이 좋습니다.

- **파티션 키(Partition Key) 선정의 어려움:**
  - 어떤 레코드를 어떤 샤드에 저장할지 결정해야 합니다.
  - 키를 알면 조회 속도가 빠르지만, 키를 모르면 모든 샤드를 뒤져야 하는 비효율적인 검색이 발생합니다.
  - 한번 정한 샤딩 스키마는 변경하기 어렵습니다.
- **데이터 모델의 제약:**
  - **Key-Value 데이터:** 키를 기준으로 샤딩하기 쉬워 잘 작동합니다.
  - **관계형(Relational) 데이터:** 보조 인덱스(Secondary Index) 검색이나 여러 샤드에 걸친 조인(Join)이 필요한 경우 구현이 까다롭습니다.
- **분산 트랜잭션의 비용:**
  - 여러 샤드에 걸쳐 데이터를 업데이트해야 할 때, 일관성을 보장하기 위해 분산 트랜잭션이 필요합니다.
  - 이는 단일 노드 트랜잭션보다 훨씬 느리고, 시스템 전체의 병목이 될 수 있으며, 아예 지원하지 않는 시스템도 있습니다.

#### 1.3. 단일 머신에서의 샤딩

특이하게도 단일 머신 내에서도 샤딩을 사용하는 경우가 있습니다.

- **목적:** CPU 코어별 병렬성을 극대화하거나 NUMA(Non-Uniform Memory Access) 아키텍처를 활용하기 위함입니다.
- **예시:** Redis, VoltDB, FoundationDB 등은 코어당 하나의 프로세스를 실행하고, 샤딩을 통해 부하를 분산합니다.

---

## 2. 멀티테넌시를 위한 샤딩 (Sharding for Multitenancy)

SaaS(Software as a Service)나 클라우드 서비스에서는 여러 고객(Tenant)이 하나의 시스템을 공유하지만, 각자의 데이터는 독립적으로 관리되어야 합니다. 이때 샤딩이 유용하게 사용됩니다.

#### 2.1. 구현 방식

- 각 테넌트에게 별도의 샤드를 할당하거나, 여러 작은 테넌트를 묶어서 하나의 큰 샤드에 할당합니다.
- 물리적으로 분리된 데이터베이스일 수도 있고, 논리적으로 분리된 영역일 수도 있습니다.

#### 2.2. 멀티테넌시 샤딩의 장점

1.  **리소스 격리 (Resource Isolation):** 특정 테넌트가 과도한 연산을 수행해도, 다른 샤드에 있는 테넌트의 성능에는 영향을 덜 줍니다.
2.  **권한 격리 (Permission Isolation):** 접근 제어 로직에 버그가 있더라도, 물리적으로 데이터가 분리되어 있으면 다른 테넌트의 데이터가 유출될 가능성이 낮아집니다.
3.  **셀 기반 아키텍처 (Cell-based Architecture):** 데이터뿐만 아니라 애플리케이션 서비스까지 테넌트 그룹별로 격리(Cell)하여, 장애가 발생해도 해당 셀 내로 피해를 국한시킬 수 있습니다(Fault Isolation).
4.  **테넌트별 백업 및 복구:** 특정 테넌트가 실수로 데이터를 삭제했을 때, 다른 테넌트에게 영향 없이 해당 테넌트의 샤드만 복구할 수 있습니다.
5.  **규제 준수 (Regulatory Compliance):** GDPR 등의 법규에 따라 특정 개인의 데이터를 삭제하거나 추출해야 할 때, 샤드 단위로 처리하면 간편합니다.
6.  **데이터 거주 요건 (Data Residence):** 특정 국가/지역에 데이터를 저장해야 하는 법적 요건이 있을 때, 해당 테넌트의 샤드를 특정 지역(Region)에 할당할 수 있습니다.
7.  **점진적 스키마 배포:** 스키마 변경을 한 번에 모든 테넌트에게 적용하지 않고, 테넌트 단위로 순차적으로 적용하여 리스크를 줄일 수 있습니다.

#### 2.3. 멀티테넌시 샤딩의 과제 (Challenges)

1.  **초대형 테넌트 문제:** 특정 테넌트의 데이터가 너무 커서 단일 노드에 들어가지 않는다면, 그 테넌트 내부에서 다시 샤딩을 해야 합니다 (일반적인 확장성 샤딩 문제로 회귀).
2.  **소규모 테넌트 관리:** 너무 많은 소규모 테넌트마다 샤드를 만들면 오버헤드가 큽니다. 이를 그룹화할 경우, 테넌트가 성장했을 때 다른 샤드로 이동시키는 마이그레이션 문제가 발생합니다.
3.  **테넌트 간 데이터 연결:** 여러 테넌트의 데이터를 연결하거나 조인해야 하는 기능이 필요할 경우 구현이 매우 어려워집니다.

## [GEMINI 보충 내용] 테넌트란??

```
용어가 참 낯설죠? IT 업계에서 쓰는 '테넌트(Tenant)'는 원래 **'세입자'**라는 뜻입니다.

이 개념을 가장 쉽게 이해하려면 **아파트**를 떠올려보세요. 텍스트로만 보면 딱딱하니까, 비유를 통해 아주 쉽게 풀어드릴게요!

---

## 1. '테넌트(Tenant)'가 정확히 뭔가요?

- **상황:** 당신이 '네이버 웍스'나 '슬랙(Slack)', '노션(Notion)' 같은 서비스를 운영하는 사장님이라고 해봅시다.
- **고객:** 삼성전자도 이 서비스를 쓰고, 동네 작은 카페 사장님도 이 서비스를 씁니다.
- **테넌트:** 여기서 **'삼성전자'라는 기업 고객 한 팀**이 바로 하나의 **'테넌트(세입자)'**입니다.

즉, **멀티테넌시(Multi-tenancy)**는 하나의 커다란 아파트(소프트웨어)에 여러 가구(고객사)가 들어가 사는 형태를 말합니다. 주차장이나 엘리베이터(CPU, 메모리 등)는 같이 쓰지만, 각자의 집 안(데이터)은 서로 볼 수 없어야 하죠.

---

## 2. '샤딩(Sharding)'은 왜 하나요?

아파트에 세입자가 너무 많아지면 어떻게 될까요? 엘리베이터가 미어터지고 소음 문제도 생기겠죠. 이때 사용하는 전략이 **샤딩**입니다.

쉽게 말해 **"고객들을 여러 개의 건물로 나누어 수용하는 것"**입니다.

### 2.1. 구현 방식 (어떻게 나누나?)

- **A동:** 삼성전자처럼 데이터가 엄청 많은 큰 손님 전용 건물.
- **B동:** 동네 카페 사장님들처럼 데이터가 적은 손님 100명이 같이 쓰는 건물.

### 2.2. 이렇게 나누면 뭐가 좋나요? (장점 요약)

1. **층간소음 방지 (리소스 격리):** 옆집(다른 테넌트)에서 파티를 열어 시끄럽게 해도(데이터를 마구 써도), 우리 집 거실은 조용합니다.
2. **보안 (권한 격리):** 우리 집 현관문이 물리적으로 따로 있으니, 옆집 사람이 실수로 우리 집 냉장고를 열어볼 일이 없습니다.
3. **부분 수리 가능 (백업/복구):** 301호에 물이 새면 301호만 수리하면 됩니다. 아파트 전체 물을 끊을 필요가 없죠.
4. **법 지키기 (규제/거주 요건):** "한국 데이터는 한국에 둬라"라는 법이 있다면, 한국 테넌트들만 모아서 한국에 있는 건물(샤드)에 살게 하면 됩니다.

### 2.3. 어려운 점은 뭔가요? (과제)

1. **너무 큰 손님:** 갑자기 한 테넌트의 가족이 1,000명으로 늘어나면, 그 집 방을 또 나눠야 합니다. (이게 다시 샤딩의 문제가 됩니다.)
2. **이사 문제:** B동에 살던 카페 사장님이 대기업이 되어 A동으로 이사가야 할 때, 그 많은 짐(데이터)을 옮기는 게 큰 일입니다.

---

## 요약하자면 이렇습니다

> **"멀티테넌시 샤딩"**이란,
> 수많은 기업 고객들을 효율적으로 관리하기 위해 **데이터베이스라는 아파트를 여러 동으로 쪼개서 손님들을 나눠 받는 기술**입니다.

이렇게 하면 한쪽이 망가져도 다른 쪽은 멀쩡하고, 각자의 사생활(데이터 보안)도 더 잘 지켜지기 때문이죠.
```

## 3. 샤딩의 목표와 문제점

- **목표:** 데이터와 쿼리 부하를 여러 노드에 **균등하게 분산**하는 것입니다. (예: 10개 노드면 10배의 성능)
- **문제점 (Skew & Hot Spot):**
  - **쏠림(Skew):** 데이터나 쿼리가 균등하지 않고 특정 샤드에 몰리는 현상.
  - **핫스팟(Hot Spot) / 핫 샤드(Hot Shard):** 불균형으로 인해 부하가 집중된 샤드.
  - **핫 키(Hot Key):** 특정 키(예: 소셜 네트워크의 유명인)에 요청이 몰리는 경우.
- **해결책:** 파티션 키를 입력받아 어느 샤드에 저장할지 결정하는 알고리즘이 필요하며, 이 알고리즘은 **재조정(Rebalancing)**이 용이해야 합니다.

---

### 3.1. 샤딩 전략 1: 키 범위 기준 샤딩 (Sharding by Key Range)

백과사전이 알파벳 순서(A-B, C-D...)로 권(Volume)이 나뉘어 있는 것과 같은 방식입니다.

- **작동 방식:**
  - 각 샤드에 연속된 키 범위(최솟값 ~ 최댓값)를 할당합니다.
  - 데이터 분포가 균일하지 않으므로(예: A로 시작하는 단어가 X로 시작하는 단어보다 많음), 범위의 간격은 데이터에 따라 다르게 설정됩니다.
- **장점:**
  - **범위 스캔(Range Scan) 효율성:** 각 샤드 내부에서 키가 정렬되어 저장되므로, `timestamp` 범위 조회 같은 쿼리가 매우 효율적입니다.
- **단점:**
  - **핫스팟 발생 위험:** 특정 범위에 쓰기가 몰릴 수 있습니다.
  - _예시:_ 키가 `timestamp`인 경우, 현재 시간 대의 샤드(예: 이번 달)에만 모든 쓰기가 몰리고 나머지 샤드는 유휴 상태가 됩니다.
  - _해결:_ `sensor_id + timestamp` 처럼 키의 앞부분에 다른 값을 붙여 분산시키지만, 이 경우 센서별로 따로 조회해야 하는 단점이 생깁니다.
- **운영 및 재조정:**
  - 초기에는 샤드가 하나뿐이므로, **Pre-splitting**(데이터 분포를 예상해 미리 샤드를 쪼개둠)을 하기도 합니다.
  - 샤드 크기가 커지거나 부하가 높으면 샤드를 둘로 쪼개고(Split), 데이터가 삭제되어 작아지면 인접 샤드와 합칩니다(Merge). (B-Tree와 유사)
- **사용 시스템:** HBase, Bigtable, MongoDB(옵션), CockroachDB 등.

---

### 3.2. 샤딩 전략 2: 키의 해시값 기준 샤딩 (Sharding by Hash of Key)

키 범위 샤딩의 쏠림(Skew) 문제를 해결하기 위해, 키 자체 대신 키의 **해시 함수 결과값**을 기준으로 샤딩합니다.

- **작동 방식:**
  - 해시 함수(MD5, Murmur3 등)를 사용해 키를 무작위 숫자로 변환합니다.
  - 유사한 키(예: 연속된 타임스탬프)라도 해시값은 완전히 다르므로 데이터가 균등하게 분산됩니다.
- **장점:** 데이터 분포가 고르게 되어 핫스팟을 방지합니다.
- **단점:** **범위 쿼리(Range Query) 효율성 상실.** 인접한 키들이 여러 샤드에 흩어지게 됩니다.

#### 3.2.1. 잘못된 접근: Modulo N (`hash(key) % N`)

- 노드 개수($N$)로 나눈 나머지를 사용하는 방식.
- **문제점:** 노드 개수($N$)가 변하면(노드 추가/삭제), **대부분의 키가 다른 노드로 이동**해야 합니다. 재조정 비용이 너무 큽니다.

#### 3.2.2 해결책 A: 고정 개수 샤드 (Fixed Number of Shards)

- **작동 방식:** 노드 수보다 훨씬 많은 수의 샤드(예: 1,000개)를 미리 생성해두고, 각 노드에 여러 샤드를 할당합니다.
  - 매핑: `hash(key) % 1,000` (샤드 개수는 변하지 않음)
- **재조정:** 노드가 추가되면, 기존 노드에서 **샤드 단위**로 데이터를 새 노드로 옮깁니다. 키-샤드 매핑은 변하지 않고, 샤드-노드 매핑만 변합니다.
- **장점:** 구현이 단순하고 재조정 시 데이터 이동 단위가 명확합니다.
- **단점:** 처음에 샤드 개수를 잘 정해야 합니다. (너무 적으면 유연성 부족, 너무 많으면 오버헤드)
- **사용 시스템:** Elasticsearch, Couchbase, Riak.

#### 3.2.3. 해결책 B: 해시 범위 샤딩 (Sharding by Hash Range) - 동적 파티셔닝

- **작동 방식:** 해시값의 전체 범위(예: $0$ ~ $2^{32}-1$)를 구간별로 나누어 샤드에 할당합니다.
- **특징:** 키 범위 샤딩처럼, 샤드가 커지면 쪼개고 작아지면 합치는 **동적 조정**이 가능합니다.
- **일관성 해싱 (Consistent Hashing):**
  - 노드가 추가/삭제될 때, 전체 키가 아닌 **최소한의 키($1/N$)만 이동**하도록 보장하는 알고리즘입니다.
  - Cassandra나 ScyllaDB는 해시 범위를 노드 개수보다 많이(예: 노드당 256개 범위) 쪼개어 무작위로 할당함으로써 부하 균형을 맞춥니다.
- **사용 시스템:** Cassandra, DynamoDB, YugabyteDB.

---

### 3.3 요약 비교

| 방식             | 기준            | 장점                         | 단점                                      | 대표 시스템         |
| :--------------- | :-------------- | :--------------------------- | :---------------------------------------- | :------------------ |
| **Key Range**    | 키 자체의 순서  | 범위 검색(Range Scan) 효율적 | 특정 패턴(예: 타임스탬프)에서 핫스팟 발생 | HBase, Bigtable     |
| **Hash (Mod N)** | 해시값 % N      | 구현 단순                    | 노드 변경 시 대규모 데이터 이동 (비효율)  | (잘 안 씀)          |
| **Fixed Shards** | 해시값 % 고정수 | 재조정 용이 (샤드 단위 이동) | 초기 샤드 개수 설정 중요                  | Elasticsearch       |
| **Hash Range**   | 해시값의 범위   | 데이터 균등 분산, 동적 확장  | 범위 검색 비효율적                        | Cassandra, DynamoDB |

## GEMINI 보충

왜 Fixed Shards를 여전히 쓸까요?
"관리의 편의성" 때문입니다.

- 🏠 비유: 이사 vs 공사
  - Fixed Shards (Elasticsearch 등): 처음부터 상자 1,000개를 준비합니다. 서버가 2대면 500개씩 나눠 갖고, 서버가 10대로 늘어나면 100개씩 나눠 갖습니다. 그냥 상자를 들고 옆집으로 옮기기만 하면 됩니다. 상자 안의 내용물을 다시 포장할 필요가 없죠.
  - Hash Range (Cassandra, MongoDB 등): 데이터가 너무 많아지면 살고 있는 집의 벽을 허물고 방을 두 개로 쪼개는 공사를 해야 합니다. 공사하는 동안 먼지도 날리고(CPU/디스크 부하), 설계도(메타데이터)도 계속 업데이트해야 하죠.
- "확장성" 하나만큼은 Hash Range가 압도적입니다.
  - 초거대 규모: 데이터가 기하급수적으로 늘어나서 처음에 정한 '고정 샤드' 개수조차 부족해질 정도라면 Hash Range가 답입니다.
  - 데이터 불균형: 특정 기간에 데이터가 몰려도 그 부분만 '동적'으로 쪼개서 대응할 수 있기 때문이죠.

제공해주신 텍스트는 **워크로드 쏠림(Skewed Workloads) 현상과 핫스팟 해결 방법**, 그리고 **샤드 재조정(Rebalancing)의 운영 방식(자동 vs 수동)**에 대한 내용입니다.

상세하게 정리해 드립니다.

---

### 4. 워크로드 쏠림과 핫스팟 완화 (Skewed Workloads and Relieving Hot Spots)

일관성 해싱(Consistent Hashing) 등을 통해 키를 노드들에 균등하게 분산하더라도, **실제 부하(Load)**가 균등하게 분산된다는 보장은 없습니다.

#### 4.1. 문제 상황: 핫스팟(Hot Spot)과 연예인 문제

- **쏠림(Skew):** 특정 키에 데이터가 집중되거나, 특정 키에 대한 요청 빈도가 압도적으로 높은 경우 발생합니다.
- **연예인(Celebrity) 문제:** 소셜 미디어에서 수백만 명의 팔로워를 가진 사용자가 글을 올리면, 해당 사용자 ID(파티션 키)로 엄청난 양의 쓰기/읽기 요청이 폭주합니다.
- **결과:** 해당 키가 저장된 샤드(노드)만 과부하가 걸리고 나머지 노드는 유휴 상태가 되는 비효율이 발생합니다.

#### 4.2. 해결책 1: 유연한 샤딩 정책 (시스템 레벨)

- 키 범위(Key Range)나 해시 범위 기반 샤딩을 사용하는 시스템에서는, **특정 핫 키(Hot Key) 하나를 독립적인 샤드로 분리**하거나, 아예 그 키만을 위한 **전용 머신**을 할당하는 방식을 사용할 수 있습니다.

#### 4.3. 해결책 2: 키 쪼개기 (애플리케이션 레벨)

시스템이 자동으로 해결해주지 못할 때, 애플리케이션 단에서 키를 인위적으로 분산시키는 방법입니다.

- **방법:** 핫 키의 뒤(또는 앞)에 **임의의 난수(Random Number)**를 붙입니다.
  - 예: `celebrity_id` -> `celebrity_id_00` ~ `celebrity_id_99`
  - 이렇게 하면 하나의 키로 몰릴 쓰기 작업이 100개의 다른 키로 분산되어 여러 샤드로 나뉩니다.
- **단점 (Trade-off):**
  - **읽기 비용 증가:** 데이터를 읽을 때 100개의 키(`_00` ~ `_99`)를 모두 조회해서 애플리케이션에서 합쳐야 합니다.
  - **관리 복잡성:** 모든 키에 이 방식을 적용하면 오버헤드가 큽니다. 따라서 어떤 키가 '핫 키'인지 추적해야 하고, 일반 키를 핫 키로 변환하는 관리 로직이 필요합니다.
- **한계:** 이 방식은 주로 **쓰기(Write) 부하**를 분산하는 데 유용하며, 읽기 부하 분산에는 다른 전략(캐싱 등)이 필요할 수 있습니다.

---

### 5. 운영: 자동 재조정 vs 수동 재조정 (Automatic or Manual Rebalancing)

샤드를 쪼개거나 노드 간에 이동시키는 '재조정' 작업을 누가 결정할 것인가는 중요한 운영 이슈입니다.

#### 5.1. 접근 방식의 종류

1.  **완전 자동 (Fully Automated):** 시스템이 부하를 감지하고 자동으로 샤드를 쪼개고 이동시킵니다. (예: DynamoDB, MongoDB 등)
2.  **수동 (Manual):** 관리자가 직접 설정을 변경하여 재조정을 수행합니다.
3.  **하이브리드 (Human in the loop):** 시스템이 재조정 계획을 제안(Suggest)하고, 관리자가 승인(Commit)해야 실행됩니다. (예: Couchbase, Riak)

#### 5.2. 자동화의 장점

- 유지보수 업무 감소.
- 워크로드 변화에 따라 자동으로 스케일링(Auto-scaling) 가능.

#### 5.3. 자동화의 위험성 (예측 불가능성)

재조정은 데이터를 대량으로 이동시키는 **비싼(Expensive)** 작업입니다. 잘못된 자동화는 시스템 전체를 위험에 빠뜨릴 수 있습니다.

- **연쇄 장애(Cascading Failure) 시나리오:**
  1.  노드 A가 과부하로 인해 응답이 일시적으로 느려짐.
  2.  시스템(자동 감지)은 노드 A를 '장애(Dead)'로 오판.
  3.  노드 A의 데이터를 다른 노드들로 옮기는 재조정(Rebalancing)을 자동 시작.
  4.  이 과정에서 발생하는 네트워크/CPU 부하가 남은 노드들에 추가됨.
  5.  남은 노드들도 과부하로 느려지고, 또다시 장애로 오판되어 재조정이 반복됨.
  6.  결국 클러스터 전체가 붕괴.

#### 5.4. 결론: 사람의 개입 (Human in the loop)

- 완전 자동화보다는 **사람이 개입하는 방식**이 운영상 '깜짝 놀랄 일(Surprises)'을 방지하는 데 좋습니다.
- 특히 블랙 프라이데이 세일이나 월드컵 같은 **예측 가능한 트래픽 급증**이 있을 때는, 자동화에 맡기기보다 관리자가 미리(Preemptively) 샤드를 늘리고 재조정해두는 것이 안전합니다.

제공해주신 텍스트는 샤딩된 데이터베이스에서 클라이언트가 올바른 노드를 찾아가는 **요청 라우팅(Request Routing)** 문제에 대한 내용입니다.

상세하게 정리해 드립니다.

---

## 6. 요청 라우팅(Request Routing)이란?

데이터를 여러 노드에 분산(샤딩)시킨 후, 클라이언트가 읽거나 쓰려는 **특정 키(Key)가 어느 노드(IP 주소 및 포트)에 있는지 찾아내는 문제**입니다.

- **서비스 디스커버리(Service Discovery)와의 차이점:**
  - 일반적인 웹 애플리케이션(Stateless): 아무 노드나 요청을 받아도 처리할 수 있으므로 단순 로드 밸런싱이 가능합니다.
  - **샤딩된 데이터베이스(Stateful):** 특정 키는 오직 해당 샤드를 보유한 노드에서만 처리할 수 있습니다. 따라서 라우팅 시스템은 **'어떤 키가 어느 샤드에 있고, 그 샤드는 어느 노드에 있는지'**를 정확히 알고 있어야 합니다.

---

### 6.1. 라우팅을 처리하는 3가지 접근 방식 (Figure 7-7)

1.  **아무 노드나 접속 (Allow clients to contact any node):**
    - 클라이언트는 라운드 로빈 등을 통해 임의의 노드에 요청을 보냅니다.
    - 만약 그 노드에 찾는 데이터가 있으면 바로 처리합니다.
    - 없으면, 그 노드가 **올바른 노드로 요청을 전달(Forwarding)**하고, 응답을 받아 클라이언트에게 반환합니다.
2.  **라우팅 계층 도입 (Routing Tier):**
    - 클라이언트는 모든 요청을 별도의 라우팅 계층(로드 밸런서 역할)으로 보냅니다.
    - 라우팅 계층은 실제 데이터를 처리하지 않으며, 파티션 매핑 정보를 보고 **올바른 노드로 요청을 토스**하는 역할만 합니다.
3.  **클라이언트가 직접 인식 (Client-Aware):**
    - 클라이언트가 파티션 매핑 정보를 가지고 있습니다.
    - 중개자 없이 **처음부터 올바른 노드로 직접 접속**합니다.

---

### 6.3. 핵심 문제: 파티션 할당 정보 관리

위의 3가지 방식 모두 **"누가 샤드 할당을 결정하고, 변경 사항(재조정 등)을 어떻게 전파할 것인가?"**라는 문제를 해결해야 합니다.

#### 6.3.1. 별도의 코디네이션 서비스 사용 (ZooKeeper / etcd)

많은 분산 시스템이 **ZooKeeper**나 **etcd** 같은 별도의 코디네이션 서비스를 사용하여 샤드 할당 정보를 관리합니다. (Figure 7-8)

- **작동 원리:**
  1.  각 노드는 ZooKeeper에 자신을 등록합니다.
  2.  ZooKeeper는 샤드-노드 매핑 정보를 유지 관리합니다 (합의 알고리즘을 통해 내결함성 보장).
  3.  라우팅 계층이나 클라이언트는 ZooKeeper를 구독(Subscribe)하여 변경 사항을 통지받습니다.
- **사용 예시:**
  - **HBase, SolrCloud:** ZooKeeper 사용.
  - **Kubernetes:** etcd 사용.
  - **MongoDB:** 독자적인 Config Server와 `mongos` 데몬(라우팅 계층) 사용.

#### 6.3.2. 내장된 합의 프로토콜 사용

외부 도구(ZooKeeper)에 의존하지 않고, 데이터베이스 자체적으로 합의 프로토콜을 내장하여 조정 기능을 수행하기도 합니다.

- **사용 예시:** Kafka, YugabyteDB, TiDB, ScyllaDB (주로 Raft 알고리즘 사용).

#### 6.3.3. 가십 프로토콜 (Gossip Protocol) 사용

- **접근 방식:** 중앙 저장소 없이 노드들끼리 서로 정보를 주고받으며(Gossip) 상태 변화를 전파합니다.
- **특징:**
  - 일관성(Consistency)이 약합니다.
  - 일시적으로 클러스터의 일부가 다른 파티션 할당 정보를 가질 수 있는 **스플릿 브레인(Split-brain)** 가능성이 있습니다.
  - 하지만 리더 없는(Leaderless) 데이터베이스는 애초에 약한 일관성을 전제하므로 이를 허용합니다.
- **사용 예시:** Riak.

---

### 6.4. 참고 사항

- **DNS의 역할:** 샤드 할당 정보는 빈번하게 바뀌지만, 노드의 IP 주소는 상대적으로 덜 바뀝니다. 따라서 클라이언트가 라우팅 계층이나 노드의 IP를 찾는 데는 DNS를 사용하는 것으로 충분합니다.
- **OLTP vs OLAP:**
  - 여기서 논의한 내용은 주로 단일 키를 찾는 **OLTP** 환경에 적합합니다.
  - 복잡한 쿼리를 수행하는 **OLAP(분석용)** 데이터베이스는 단일 샤드가 아니라, 수많은 샤드에서 병렬로 데이터를 읽어 집계하는 방식(MPP 등)을 사용하며 이는 11장에서 다룹니다.

### 7.1. 문제 제기: 보조 인덱스와 샤딩의 부조화

- **기본 샤딩:** 파티션 키(Partition Key)를 기준으로 데이터를 나눕니다. 키-값 모델에서는 파티션 키를 알면 해당 샤드로 바로 접근할 수 있어 효율적입니다.
- **보조 인덱스의 필요성:** "사용자 123의 모든 활동", "빨간색 자동차 찾기" 등 파티션 키가 아닌 다른 속성으로 검색해야 할 때가 많습니다. (관계형 DB, 문서형 DB, 검색 엔진의 핵심 기능)
- **난관:** 보조 인덱스는 파티션 키와 무관한 값들을 기준으로 하므로, 샤딩된 구조에 깔끔하게 매핑되지 않습니다.

---

### 7.2. 접근법 1: 지역 보조 인덱스 (Local Secondary Indexes)

각 샤드가 **자신이 보유한 데이터에 대해서만** 인덱스를 생성하고 관리하는 방식입니다.
(정보 검색 분야에서는 **Document-partitioned index**라고도 부릅니다.)

- **구조 (Figure 7-9):**
  - 샤드 0은 샤드 0에 있는 '빨간 차' 정보만 인덱싱합니다.
  - 샤드 1은 샤드 1에 있는 '빨간 차' 정보만 인덱싱합니다.
- **쓰기(Write) 효율성:**
  - 데이터를 추가/수정할 때, 해당 데이터가 속한 샤드 하나만 건드리면 됩니다. 인덱스 업데이트도 그 샤드 내부에서만 일어납니다.
- **읽기(Read) 비효율성 (Scatter/Gather):**
  - "빨간 차를 찾아라"라는 쿼리가 오면, 클라이언트는 **모든 샤드**에 쿼리를 보내야 합니다(Scatter).
  - 각 샤드는 자신의 인덱스에서 결과를 찾아 반환하고, 클라이언트는 이를 취합(Gather)해야 합니다.
  - **단점:**
    - 모든 샤드에 질의해야 하므로 **테일 레이턴시(Tail Latency)**가 증가할 위험이 큽니다. (가장 느린 샤드가 전체 응답 속도를 결정)
    - 샤드를 늘려도 쿼리 처리량(Throughput)이 선형적으로 증가하지 않습니다. (모든 샤드가 모든 쿼리를 처리해야 하므로)
- **사용 시스템:** MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud, VoltDB 등.

---

### 7.3. 접근법 2: 전역 보조 인덱스 (Global Secondary Indexes)

모든 샤드의 데이터를 아우르는 하나의 큰 인덱스를 구성하되, 이 **인덱스 자체를 다시 샤딩**하여 분산 저장하는 방식입니다.
(정보 검색 분야에서는 **Term-partitioned index**라고도 부릅니다.)

- **구조 (Figure 7-10):**
  - '색상' 인덱스를 예로 들면, `color:red`에 해당하는 모든 자동차 ID(어느 샤드에 있든 상관없이)를 모아서 저장합니다.
  - 이 인덱스 데이터가 너무 크므로, 인덱스 자체를 또 나눕니다. (예: 색상 이름이 A~R로 시작하면 노드 A에, S~Z로 시작하면 노드 B에 저장)
- **읽기(Read) 효율성:**
  - "빨간 차(color=red)를 찾아라"라는 쿼리가 오면, `red`를 담당하는 **특정 샤드 하나**에만 요청하면 됩니다. (Scatter/Gather 불필요)
- **쓰기(Write) 복잡성:**
  - 데이터 한 건을 쓸 때, 그 데이터의 여러 속성(색상, 제조사 등)에 대한 인덱스가 서로 다른 샤드에 흩어져 있을 수 있습니다.
  - 따라서 쓰기 작업이 **여러 샤드에 걸친 분산 트랜잭션**을 유발할 수 있어 느리고 복잡합니다.
  - 대안으로 DynamoDB 등은 **비동기 갱신**을 사용하여, 인덱스 읽기 결과가 최신 데이터와 일치하지 않을 수 있음(Eventual Consistency)을 허용하기도 합니다.
- **사용 시스템:** CockroachDB, TiDB, YugabyteDB, DynamoDB(Local/Global 둘 다 지원).

---

### 7.4. 요약 비교

| 특징            | 지역 보조 인덱스 (Local)                     | 전역 보조 인덱스 (Global)                              |
| :-------------- | :------------------------------------------- | :----------------------------------------------------- |
| **인덱스 범위** | 각 샤드 내부 데이터만 포함                   | 전체 샤드 데이터 포함 (인덱스 자체를 샤딩)             |
| **다른 이름**   | Document-partitioned                         | Term-partitioned                                       |
| **쓰기 성능**   | **빠름** (해당 샤드만 업데이트)              | **느림/복잡** (여러 샤드 업데이트 필요, 분산 트랜잭션) |
| **읽기 성능**   | **느림** (모든 샤드에 질의 - Scatter/Gather) | **빠름** (해당 인덱스 샤드만 질의)                     |
| **주요 용도**   | 쓰기 위주, 단순 구조 선호 시                 | 읽기 위주, 빠른 검색 속도 필요 시                      |

## 8. 마무리

### 1. 샤딩의 목적과 필요성

- **필요성:** 데이터의 양이 너무 많거나 처리 부하가 너무 커서, 단일 머신에서 저장하고 처리하는 것이 불가능할 때 필수적입니다.
- **목표:**
  - 데이터와 쿼리 부하를 여러 머신에 **균등하게 분산(Evenly spread)**하는 것.
  - 데이터가 한 곳에 쏠리는 **핫스팟(Hot Spot)**을 방지하는 것.
- **요구사항:** 데이터 특성에 맞는 적절한 샤딩 전략을 선택해야 하며, 노드가 추가/제거될 때 샤드를 재배치하는 **재조정(Rebalancing)**이 필요합니다.

---

### 2. 두 가지 주요 샤딩 전략

#### A. 키 범위 샤딩 (Key Range Sharding)

- **방식:** 키들이 정렬된 상태로 저장되며, 각 샤드는 특정 범위(최솟값 ~ 최댓값)의 키를 담당합니다.
- **장점:** **범위 쿼리(Range Query)**가 매우 효율적입니다.
- **단점:** 정렬된 순서로 인해, 인접한 키에 접근이 몰릴 경우 **핫스팟**이 발생할 위험이 있습니다.
- **재조정:** 특정 샤드가 너무 커지면 범위를 반으로 쪼개는(Splitting) 방식을 주로 사용합니다.

#### B. 해시 샤딩 (Hash Sharding)

- **방식:** 각 키에 해시 함수를 적용하고, 그 해시값을 기준으로 샤드를 할당합니다.
- **장점:** 키의 순서를 섞어버리므로 데이터와 부하를 **더 균등하게 분산**할 수 있습니다.
- **단점:** 키의 정렬 순서가 파괴되므로 **범위 쿼리가 비효율적**입니다.
- **재조정:**
  - 미리 많은 수의 샤드를 생성해두는 **고정 개수 샤드(Fixed number of shards)** 방식을 흔히 사용합니다.
  - 노드가 추가/제거되면 샤드 전체를 이동시킵니다.
  - 일관성 해싱(Consistent Hashing)을 사용하기도 합니다.

#### C. 복합 방식 (Hybrid)

- **방식:** 키의 **첫 부분**은 파티션 키(해싱하여 샤드 결정)로 사용하고, **나머지 부분**은 정렬 키로 사용하여 샤드 내부에서 데이터를 정렬합니다.
- **효과:** 동일한 파티션 키를 가진 데이터들 사이에서는 효율적인 범위 쿼리가 가능합니다.

---

### 3. 샤딩과 보조 인덱스 (Secondary Indexes)

보조 인덱스도 샤딩이 필요하며, 두 가지 접근법이 있습니다.

#### A. 지역 보조 인덱스 (Local Secondary Indexes)

- **방식:** 각 샤드가 **자신의 데이터에 대한 인덱스만** 관리합니다.
- **특징:**
  - **쓰기(Write):** 빠름. (데이터가 저장된 샤드 하나만 업데이트하면 됨)
  - **읽기(Read):** 느림. (모든 샤드에 쿼리를 보내 결과를 취합해야 함 - Scatter/Gather)

#### B. 전역 보조 인덱스 (Global Secondary Indexes)

- **방식:** 인덱스 자체를 별도로 샤딩하여 관리합니다. (인덱스 키를 기준으로 샤딩)
- **특징:**
  - **읽기(Read):** 빠름. (인덱스를 가진 특정 샤드만 조회하면 됨)
  - **쓰기(Write):** 느리고 복잡함. (데이터 한 건을 쓸 때, 여러 인덱스 샤드를 업데이트해야 할 수 있음)

---

### 4. 운영 및 라우팅

- **요청 라우팅(Request Routing):** 클라이언트가 원하는 키가 어느 노드에 있는지 찾아가는 방법이 필요합니다.
- **코디네이션(Coordination):** ZooKeeper 같은 별도의 서비스를 사용하여 샤드와 노드 간의 할당 정보를 관리하고 추적하는 경우가 많습니다.

---

### 5. 결론 및 다음 장 예고

- 샤딩된 데이터베이스의 각 샤드는 기본적으로 **독립적**으로 작동하며, 이것이 수평적 확장을 가능하게 하는 핵심입니다.
- 하지만 **여러 샤드에 걸쳐서 데이터를 써야 하는 작업(Multi-shard writes)**은 원자성(Atomicity)이나 일관성 문제가 발생할 수 있어 까다롭습니다. (예: 한 샤드는 성공하고 다른 샤드는 실패하면?)
- 이러한 문제는 다음 장들(트랜잭션, 분산 시스템의 문제 등)에서 다루게 됩니다.
