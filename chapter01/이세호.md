오늘날 많은 애플리케이션은 **연산 집약적(compute-intensive)**이기보다 **데이터 집약적(data-intensive)**이다. 문제의 핵심은 CPU 성능이 아니라 데이터의 양, 복잡성, 변화 속도에 있다.

다양한 데이터 시스템들이 처리 방식에 따라 효율성을 위해 구간 마다 나누어서 데이터를 관리함.

설계 고려할 때 가장 핵심 세 가지: Reliability, Scalability, Maintainability

왜 이 세 가지인가? - "시간이 지나도 시스템이 살아남기 위한 최소 조건"

- Reliability: 오늘 제대로 동작하는가
- Scalability: 내일 더 커져도 동작하는가
- Maintainability: 모레 사람이 바뀌어도 고칠 수 있는가

## 1. Reliability (신뢰성)

### 1-1. 서문

#### Fault와 Failure의 차이

- Fault(결함): 시스템의 한 구성 요소가 명세에서 벗어나 동작하는 상태
- Failure(장애): 시스템 전체가 사용자에게 필요한 서비스를 제공하지 못하는 상태

모든 fault를 완전히 제거하는 것은 불가능하므로,
핵심은 fault가 failure로 이어지지 않도록 설계하는 것이다.
=> 결론: 완전 다 망가지는건 무조건 피해야하고, 일부 시스템이 영향을 조금 받는 정도로 최소화하는 것이 목표.

#### 일부러 주기적인 fault 발생

설적으로, 신뢰적인 시스템에서는 의도적으로 fault를 발생시키는 것이 도움이 될 수 있다.

- 많은 치명적 버그는 오류 처리 미흡에서 발생
- 일부러 장애를 유발하면 내결함성 로직이 지속적으로 테스트됨
- 실제 장애 상황에서도 올바르게 동작할 것이라는 신뢰가 높아짐
- 대표적인 예가 Netflix의 Chaos Monkey로, 무작위로 프로세스를 종료시켜 시스템의 회복 능력을 검증한다.

#### 예방이 더 중요한 경우도 있음

- 일반적으로는 fault를 허용하고 복구하는 전략이 선호되지만,
- 복구가 불가능한 경우에는 예방이 최선이다.
- 대표 사례: 보안 침해
- 민감한 데이터 유출은 사후 복구가 불가능
- 보안 버그 같은 것은 아예 한 번 실수하면 끝인데, 이런거는 책 스코프에서는 제외임.
- 이 책은 주로 복구 가능한 fault를 다루며, 불완전한 구성 요소들로도 신뢰적인 시스템을 구축하는 방법을 설명한다.

### 1-2. Hardware Faults

- 하드디스크의 평균 고장 간격(MTTF)은 약 10~50년으로 보고됨.
- 디스크가 10,000개 있는 스토리지 클러스터에서는 하루에 평균 1개 디스크 고장이 발생할 수 있다.
- 고로, 대규모 데이터센터에서는 이런 문제들이 상시적으로 발생한다.

- 과거에는 서버가 사내에 보통 한대여서 기초적인 복제(redundancy) 정도로 충분했음. (RAID, 핫스왑 CPU, 듀얼 전원 공급 장치, 배터리/디젤 발전기 백업 전기 등)
- 하지만 현대에 들어서는 데이터 규모가 많이 커져서, 내결함성을 깔고 들어가야하도록 바뀌었음.

### 1-3. Software Errors

- 하드웨어는 전체가 오류가 나는 경우는 매우 드뭄. (인스턴스 하나 다운되어도 load balancer에 몇 개 더 남아있음.)
- 소프트웨어에서의 결함은 보통 전체 시스템에 파급력이 있음. (버그는 전체 배포 버전에 동일하게 모두 영향을 미침.) 그래서 훨씬 큰 장애로 이어질 가능성이 높음.

꽤 자주 나오는 패턴 - 엣지 케이스 잠복.

체계적 소프트웨어 오류에는 단번에 해결되는 방법은 없다.
대신 여러 작은 노력들의 조합이 필요하다.

- 파훼법 (완전한 해결책은 없음):
  - 시스템 가정과 컴포넌트 간 상호작용을 신중히 설계
  - 철저한 테스트 (특히 경계 조건)
  - 프로세스 격리
  - 장애 발생 시 크래시 후 재시작 허용
  - 운영 환경에서의 측정, 모니터링, 로그 분석

### 1-4. Human Errors

- 1위 오류 = 대부분은 사람의 설정(config) 오류. 하드웨어 오류는 10~25% 뿐.

파훼법:

1. 실수를 유발하기 어려운 설계
   - 잘 설계된 추상화, API, 관리자 인터페이스
   - “올바른 사용”은 쉽게, “잘못된 사용”은 어렵게
   - 단, 너무 제한적이면 우회 사용이 생기므로 균형이 중요
   - => 내 생각: 디자인 시스템 등과 같은 것도 나옴. 거의 모든 설계를 아우르는 느낌의 것이라 꽤 넓은 범위인듯함.
2. 실험과 운영 환경의 분리
   - 실수가 잦은 작업을 실제 장애로 직결되지 않게 분리
   - 실제 데이터로 실험 가능한 비운영(Sandbox) 환경 제공
   - 운영 환경에 영향을 주지 않고 안전한 실험 가능
   - => 내 생각: 이건 매우 좋은듯함. dev/staging에서 잡히는 경우 매우매우 high.
3. 철저한 테스트
   - 단위 테스트 → 통합 테스트 → 전체 시스템 테스트 → 수동 테스트
   - 자동화 테스트는 잘 알려져 있고 특히 드물게 발생하는 경계 상황을 검증하는 데 효과적
   - => 내 생각: config는 테스팅을 하기 어렵지 않은가..? production에서만 적용되는 config들이 진짜 제일 취약한듯함. 이걸 테스트로 관리가 가능한가? 테스트라기 보다는 staging 서버가 실제 production이랑 얼마나 비슷하냐에 달린듯함. 비즈니스 로직 버그는 무조건 없음을 보장해야함. 그렇기에 테스팅이 언급되었다고 생각함.
4. 빠른 복구를 가능하게 하는 설계
   - 설정 변경 신속 롤백
   - 새 코드의 점진적 배포 (문제 발생 시 영향 최소화)
   - 잘못된 계산을 바로잡을 수 있는 데이터 재계산 도구 제공
5. 모니터링과 관측 가능성
   - 성능 지표, 에러율 등 정교하고 명확한 모니터링
   - 다른 공학 분야에서는 이를 **텔레메트리(telemetry)**라 부름
   - 이상 징후 조기 감지
   - 가정과 제약 조건 위반 여부 확인
   - 장애 발생 시 원인 분석에 핵심적
6. 관리와 교육
   - 좋은 관리 체계와 지속적인 교육 역시 중요
   - 다만 이는 기술적 주제 범위를 넘어 이 책에서는 깊이 다루지 않음

### 1-5. How Important Is Reliability?

- 항공, 미사일 등 예민한 분야에만 신뢰성이 중요한 것이 아님.
- 일상적인 드라이브 어플리케이션 등도 신뢰성이 중요함.
- 아기 사진을 드라이브에 유일하게 보관했는데, 그 데이터가 날아가면 그것 또한 치명적인 이슈로 번질 수 있음.
- 고로, 일상적인 애플리케이션에서도 신뢰성은 매우 중요하다.
  - 비즈니스 애플리케이션의 버그
    → 생산성 손실, 잘못된 수치 보고 시 법적 리스크
  - 전자상거래 사이트 장애
    → 직접적인 매출 손실과 브랜드 신뢰도 하락

결론: 매우매우 중요.

## 2. Scalability (확장성)

### 2-1. Describing Load

이 파트의 핵심: "load parameters"(부하 파라미터)를 잘못 잡으면 망함.

- 로드 변수가 뭐지..?

트위터 예시를 표면적으로만 보면:

주요 작업

- 트윗 작성(Post tweet)
  - 평균 4,600 req/s
  - 피크 12,000 req/s
- 홈 타임라인 조회(Home timeline)
  - 약 300,000 req/s

쓰기 자체(12k/s)는 어렵지 않지만,
문제의 핵심은 fan-out(팔로워 수에 따른 확산) 이다.

접근 방식 1: 읽기 시점에 계산 (Read-time fan-out)
접근 방식 2: 쓰기 시점에 계산 (Write-time fan-out)

트위터는 처음에는 방식 1이었다가, 팔로워 수 몇 명 안되는 일반인들은 방식 2로 바꾸고, 셀럽들은 접근 1로 유지하는 방식 채택.

**핵심 부하 파라미터: 사용자별 팔로워 수 분포**

핵심 부하 파라미터를 잘못 잡는다면 설계가 잘못될 가능성 매우 높아짐.

내 생각 추가 핵심: system design은 roi를 따지는 리스크 매니지먼트에 가까움. 성능은 적당히 좋아도 됨. 수많은 변수를 생각해야함.

### 2-2. Describing Performance

- 이 파트의 핵심: 부하 파라미터를 잘못된 방식으로 해석하면 또 망한다.

- 시스템 유형별 성능 지표
  - 배치 처리 시스템 (예: Hadoop)
    - 처리량(throughput): 초당 처리 레코드 수 또는 전체 작업 완료 시간
  - 온라인 시스템
    - **응답 시간(response time)**이 핵심
    - 요청을 보낸 시점부터 응답을 받기까지의 시간

응답 시간은 단일 값이 아니라 분포임. 그게 중요함.

엥 무슨 말임?

결국은, 두 가지를 조심해야함:

1. 응답 시간을 대표하는 단일 지표를 찾아서 보려는 노력. => 분포를 파악하는데에 방해가 될 수 있음. 분포 파악을 위해 여러 지표를 봐야함.
2. 분포 파악을 위해 보는 지표들도 잘 선별해야함.

더 자세히 알아보면 바로 이해됨.

평균 응답 시간 (mean) - 쓰레기 값임. 왜냐하면 uniform distribution이 아니기 때문에. response time은 느린 애들이 극소수로 몰려있는 지수/로그 함수 느낌의 그래프일 가능성이 높음. 그래서 소득 평균 계산처럼 쓰레기 값임.

p50(중앙값), p95, p99를 봐야함. - 좋은 값인데, 여러개를 고루 봐야함. 꼭 하나만 빠르게 본다면 p99 기준이 다른 것 보다 나음. 왜냐하면 가장 불편한 사용자이기 떄문에. 그리고, AWS에서는 그 가장 불편한 사용자가 보통 가장 데이터가 많은 헤비 유저이고, 돈을 가장 많이 벌어주는 고객이기에 매우 중요하게 취급한다고함. 그렇기에, p99 기준으로 엄격하게 바라봐야함. 회사 스케일에 따라 p99.9까지도 봐야할 수 있는데, p99.99는 엔터프라이즈 급에서도 잘 안본다고함. 이유는, P99.99 까지 가면 보통 만 번에 한 번 발생하는 아주 특수 케이스일 경우가 많다고 함. (반복 패턴이 없어서 고도화하기에 돈이 너무 많이 드는 특수 예외들.)

또 주의할점들:

- 백엔드 서버 여러개일 때 Percentile 끼리 평균내면 안됨. => 분포 자체를 합치는게 맞음. (== 히스토그램 합치기)
- 부하 테스트할 때 응답 기다리지말고 계속 보내셈 (이건 k6 쓰면 자동 처리)
- SLA, SLO에서 지표로 쓸 수 있음. SLA에서 못 지키면 고객들에게 환불/보상할 기준이 됨.

### 2-3. Approaches for Coping with Load

- 한 부하 수준에 적합한 아키텍처는 10배 부하에서는 거의 확실히 통하지 않는다.
- 빠르게 성장하는 서비스라면 부하가 한 자릿수 배 증가할 때마다 아키텍처 재검토가 필요 (최근 github 리드 개발자 팟캐스트에서 본 내용 - 0하나 붙인 것 까지는 생각함. 근데 그 이상은? 당장은 생각안 할 가능성 높음.)

- Scale Up/Out => 한쪽만 쓰지 않음. ec2 200개 보다 적당히 큰 인스턴스 몇 개만 쓰는게 더 안정적임. => 즉, 둘이 밸런스를 맞추는 편임.

- 자동 확장(Elastic Load Balancing)이 꼭 좋은 것은 아님. 수동 확장도 오히려 예측 못할 Load Balancer 오토 파일럿 이슈를 줄여서 좋을 수 있음.

## 3. Maintainability (유지보수성)

- 소프트웨어 비용의 대부분은 초기 개발이 아니라 유지보수에서 발생한다.
- 유지보수에는 다음이 포함된다:
  - 버그 수정
  - 시스템 운영 및 장애 대응
  - 장애 원인 분석
  - 새로운 플랫폼 대응
  - 새로운 요구사항/유즈케이스 추가
  - 기술 부채 상환
  - 기능 확장
- 많은 개발자들이 레거시 시스템 유지보수를 싫어하는데, 그 이유는:
  - 다른 사람이 만든 복잡한 코드
  - 더 이상 최신이 아닌 기술 스택
  - 원래 의도와 다르게 확장된 시스템 때문

레거시 시스템은 각자 다르게 불쾌하기 때문에 만능 해결책은 없다.
하지만 애초에 레거시를 만들지 않도록 설계하는 것은 가능하다.
이를 위해 시스템 설계 시 다음 3가지 원칙에 집중한다:

- Operability (운영 용이성)
- Simplicity (단순성)
- Evolvability (진화 가능성)

### 3-1. Operability: Making Life Easy for Operations

- 이 파트의 핵심: 운영 용이성은 깔끔한 옵저버빌리티, 이해하기 쉬운 인프라가 80% 이상을 차지함.

### 3-2. Simplicity: Managing Complexity

- 이 파트의 핵심:

  - 시스템 레벨에서의 개념 수 최소화를 해야한다. (good abstractions)
  - 깔끔한 코드와 파일 구조가 유지보수성을 좌우한다.
  - 필요 없는 복잡도를 제거하는 모든 행위가 Simplicity

- 내 생각:
  - 사실상 모든 코드 상의 가독성/디자인 패턴이 다 이 항목에 들어가는듯함.
  - good abstractions => 디자인 패턴이라고 생각해봤는데, 알고보니 시스템 레벨에서의 개념 수가 핵심인듯함.

### 3-3. Evolvability: Making Change Easy

- 이 파트의 핵심:

  - 큰 시스템도 코드/기능 수정처럼 변화될 수 있어야함.
  - Evolvability란 “변경이 일어날 것을 전제로 한 설계 능력”이다.

- 기능/코드 수정은 TDD/애자일 등의 이론으로 많이 다루어짐. 매우 흔한 토픽.
- 필자왈: 대규모 디자인 시스템의 아키텍처도 똑같이 쉽게 마이그레이션 가능해야함. 트위터의 방법론 1, 2 예시처럼.
- 책에서 계속 다룰 내용이라고 나옴.
