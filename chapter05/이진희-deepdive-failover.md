# Deepdive: 장애 복구(Failover)의 어두운 면 - 실제 사례 분석

---

## 1. 들어가며

리더 기반 복제(Leader-based Replication)에서 리더가 장애를 일으키면, 팔로워 중 하나를 새로운 리더로 승격시키는 **장애 복구(Failover)** 가 필요하다. 단순해 보이는 이 과정에는 수많은 함정이 숨어 있다.

이 문서에서는 **실제 장애 사례**를 중심으로 장애 복구 과정에서 발생할 수 있는 문제들을 분석한다.

### 장애 복구의 3단계 (간략 요약)

| 단계 | 설명 | 함정 |
|------|------|------|
| 1. 리더 장애 감지 | 타임아웃으로 리더 사망 판단 | 네트워크 지연 vs 실제 장애 구분 불가 |
| 2. 새 리더 선출 | 합의 알고리즘 또는 제어 노드가 선출 | 스플릿 브레인 가능성 |
| 3. 시스템 재설정 | 클라이언트가 새 리더로 쓰기 전송 | 이전 리더 복귀 시 충돌 |

이론적인 내용은 책을 참고하고, 아래에서는 **실제로 무엇이 잘못됐는지** 사례를 통해 살펴본다.

---

## 2. 사례 1: GitHub 2012 - STONITH 실패와 양쪽 노드 활성화

### 2.1 사건 개요

| 항목 | 내용 |
|------|------|
| 일시 | 2012년 12월 22일 (토요일) |
| 영향 | 약 9시간 23분 서비스 장애 (11:00~20:23 PST) |
| 원인 | 네트워크 스위치 업그레이드 중 예기치 않은 장애 복구 트리거 |

### 2.2 무슨 일이 있었나?

GitHub은 파일서버를 **고가용성(HA) 클러스터**로 운영하고 있었다. 각 파일서버 쌍은 Active-Passive 구조로, 장애 시 STONITH(Shoot The Other Node In The Head)를 통해 상대 노드를 강제 종료하고 자신이 Active가 되는 방식이었다.

> **Active-Passive 구조란**
>
> 두 개 이상의 시스템 중 **하나(Active)만 실제로 요청을 처리**하고,
> 나머지(Passive)는 **데이터를 동기화하며 대기**하다가
> Active에 장애가 발생하면 **Passive가 대신 서비스하는 고가용성 구조**다.
> 
> **STONITH(Shoot The Other Node In The Head)**
> 
> 클러스터에서 어떤 노드가 문제가 있다고 판단되면,
> 그 노드를 ‘확실하게 강제 종료’해서
> 다시는 서비스에 관여하지 못하게 만드는 메커니즘
>

```
[정상 상태]
┌─────────────┐     하트비트     ┌─────────────┐
│ 파일서버 A  │ ←──────────────→ │ 파일서버 B  │
│  (Active)   │                  │  (Passive)  │
└─────────────┘                  └─────────────┘
```

문제는 **네트워크 집계 스위치(Aggregation Switch)** 업그레이드 중에 발생했다.

### 2.3 장애 과정

```
[타임라인]

1. 네트워크 스위치 소프트웨어 업그레이드 시작

2. MLAG(Multi-Chassis Link Aggregation) 페어에서 이상 발생
   - 에이전트 종료 시 피어 간 링크가 다운되지 않음
   - 하트비트는 사라졌는데 링크는 살아있는 것처럼 보임

3. 파일서버들이 상대방의 하트비트 타임아웃 감지
   - "상대방이 죽었다!" → STONITH 명령 발송

4. 네트워크 혼란으로 STONITH 명령 일부 미전달
   - 상대방을 죽이려 했는데 명령이 안 감

5. 결과: 여러 파일서버 쌍에서 "양쪽 노드 모두 Active" 상태!
```
> **MLAG(Multi-Chassis Link Aggregation)**
> 
> MLAG는 두 대의 스위치를 하나처럼 동작시켜
> 고가용성과 성능을 동시에 얻기 위한 기술이지만,
> Peer 간 통신이 끊기면 심각한 장애로 이어질 수 있다.
> 


### 2.4 왜 위험한가? - 스플릿 브레인

```
[스플릿 브레인 발생]

┌─────────────┐                  ┌─────────────┐
│ 파일서버 A  │   네트워크 단절   │ 파일서버 B  │
│  (Active)   │ ────── X ────── │  (Active)   │
│ "내가 주인" │                  │ "내가 주인" │
└─────────────┘                  └─────────────┘
       ↓                                ↓
   쓰기 처리                        쓰기 처리
       ↓                                ↓
     데이터 A                        데이터 B

→ 네트워크 복구 시 데이터 충돌/손실
```

### 2.5 복구 과정

5시간에 걸친 수동 복구가 필요했다:

1. 소프트웨어를 이전 버전으로 **다운그레이드**
2. 각 파일서버 쌍을 개별적으로 점검
3. 로그를 분석해서 **어느 쪽이 진짜 Active였는지** 판단
4. 잘못된 쪽을 수동으로 Passive로 전환
5. 전체 파일 스토리지 인프라 재시작 및 검증

### 2.6 교훈

> **STONITH는 "반드시 성공해야 하는" 명령인데, 실패할 수 있다.**

| 문제 | 교훈 |
|------|------|
| 네트워크 불안정 시 STONITH 명령 미전달 | 펜싱은 여러 방법을 조합해야 함 (전원, 네트워크, 스토리지) |
| 스위치 업그레이드가 HA 시스템에 영향 | 네트워크 변경 전 HA를 유지보수 모드로 |
| 자동 장애 복구가 상황을 악화 | 클러스터 통신은 독립된 네트워크로 분리 |

### 2.7 이후 개선 사항

GitHub은 다음을 적용했다:
- 스위칭 업그레이드 전 **스테이징 환경에서 완전 테스트** 의무화
- 네트워크 변경 시 **파일서버 HA 소프트웨어를 유지보수 모드**로 전환
- 클러스터 통신을 **독립적인 인프라로 분리** 추진

---

## 3. 사례 2: GitLab 2017 - 복제 지연, 백업 실패, 그리고 rm -rf

### 3.1 사건 개요

| 항목 | 내용 |
|------|------|
| 일시 | 2017년 1월 31일 |
| 영향 | 약 18시간 서비스 중단, 6시간 분량의 데이터 영구 손실 |
| 원인 | PostgreSQL 복제 지연 → 수동 복구 시도 중 실수 → 백업 전부 실패 |

GitLab은 주(primary) DB와 보조(secondary) DB로 구성되어 있음
보조 DB는 장애 시 대체용으로 준비되어 있지만, 당시엔 업데이트가 지연되어(primary보다 늦음) 문제가 발생

### 3.2 상세 타임라인

```
[2017년 1월 31일]

17:20 UTC - 스테이징 환경에 프로덕션 DB의 LVM 스냅샷 로드
            (이게 나중에 유일한 복구 수단이 됨)

19:00 UTC - 스팸 공격으로 데이터베이스 부하 급증
            + 스팸 계정 정리용 자동 잡(background job)이 실수로 
               GitLab 직원 계정을 스팸으로 인식하여 그 계정과 연관된 모든 데이터 삭제 작업 시작
            → 두 작업이 겹쳐서 부하 폭증

23:00 UTC - 부하증가로 인해 PostgreSQL 복제가 지연되기 시작
            → 복제 완전 실패
            "WAL segments needed by the secondary were already removed"
            (보조 서버가 필요로 하던 WAL 세그먼트가 이미 주 서버에서 삭제되었기 때문)

23:30 UTC - 엔지니어가 복제 복구를 위해 보조 서버의 데이터 디렉터리 삭제 시도

            의도: rm -rf /var/opt/gitlab/postgresql/data  (보조 서버)
            실수: rm -rf /var/opt/gitlab/postgresql/data  (주 서버!!!)

            약 2초 후 실수 인지 후 프로세스 중단 -> 이미 300GB 데이터 삭제됨
```

### 3.3 왜 이런 실수가 발생했나?

```
[터미널 창 상황]

┌──────────────────────────────────────┐
│ [터미널 1] db1.cluster.gitlab.com    │  ← 주 서버
│ $ rm -rf /var/opt/gitlab/postgre...  │  ← 여기서 실행!
└──────────────────────────────────────┘

┌──────────────────────────────────────┐
│ [터미널 2] db2.cluster.gitlab.com    │  ← 보조 서버
│ $                                     │  ← 여기서 실행하려 했음
└──────────────────────────────────────┘

문제: 두 터미널이 거의 동일하게 생겼다!
```

### 3.4 백업은? - 5가지 방법 모두 실패

| 백업 방법 | 결과 | 실패 이유 |
|-----------|------|-----------|
| **pg_dump** | ❌ 실패 | PostgreSQL **9.2**로 **9.6** DB 백업 시도 → 버전 불일치 오류. 오류 알림 이메일은 **DMARC 설정 오류**로 전달 안 됨 |
| **LVM 스냅샷** | ⚠️ 부분 성공 | 재해복구용이 아닌 **스테이징 테스트용** 목적. 6시간 전 스냅샷만 존재 |
| **Azure 디스크 스냅샷** | ❌ 미설정 | DB 서버에 **활성화되지 않음** |
| **PostgreSQL 복제** | ❌ 실패 | 복제 지연으로 **이미 데이터 유실 상태** |
| **자동 백업** | ❌ 실패 | **담당자 부재**로 정기적 복구 테스트 미시행 → 실패 사실 모름 |

### 3.5 데이터 손실 규모

```
손실 기간: 17:20 UTC ~ 00:00 UTC (약 6시간 40분)

├── 약 5,000개 프로젝트
├── 약 5,000개 댓글
├── 약 700개 신규 사용자 계정
└── 이슈, 머지 리퀘스트 등

다행히 Git 저장소와 Wiki는 별도 저장소라 손실되지 않음
```

### 3.6 복구 과정의 특이점 - 투명성

GitLab은 복구 과정을 **YouTube로 생중계**했다. 피크 시점에 5,000명 이상이 시청했다.

> "엄청 창피한 상황이었지만, 투명하게 공개함으로써 오히려 신뢰를 얻었다."

### 3.7 교훈

#### 복제는 백업이 아니다

```
[흔한 오해]
"복제 서버가 있으니까 백업은 필요 없어"

[현실]
- 복제 서버도 지연될 수 있다
- 주 서버에서 DELETE 실행하면 복제 서버도 DELETE
- rm -rf는 복제와 무관하게 즉시 삭제
```

#### 백업은 반드시 테스트해야 한다

```python
# 안티 패턴
def backup():
    pg_dump(database)
    # 끝. 복구 테스트? 그건 뭐지?

# 올바른 패턴
def backup():
    pg_dump(database)

def weekly_test():
    restore(backup_to_test_env)
    run_integrity_checks()
    alert_if_failed()
```

#### 터미널 구분을 명확히

```bash
# 위험: 프로덕션과 스테이징이 똑같이 생김
$ hostname
db1.cluster.gitlab.com

# 개선: 프롬프트에 환경 표시
PRODUCTION [db1]$
STAGING [db2]$

# 또는 프로덕션은 빨간색 배경 등
```

### 3.8 이후 개선 사항

| 영역 | 개선 사항 |
|------|-----------|
| 백업 | 시간당 LVM 스냅샷, Azure 디스크 스냅샷 활성화 |
| 복제 | PostgreSQL WAL 아카이빙, 다중 보조 서버 (pgpool-II) |
| 테스트 | 백업 복구 **자동화 테스트** 구현 |
| 책임 | **데이터 내구성 담당자** 지정 |

---

## 4. 사례 3: Cloudflare 2020 - 리더 선출 실패

### 4.1 사건 개요

| 항목 | 내용 |
|------|------|
| 일시 | 2020년 11월 2일 |
| 영향 | 약 6시간 33분 동안 API/대시보드 장애 (API 요청 최대 25% 실패) |
| 원인 | 네트워크 파티션으로 인한 RAFT 리더 선출 실패 |

### 4.2 무슨 일이 있었나?

Cloudflare의 컨트롤 플레인은 **etcd 클러스터**를 사용해서 설정과 상태를 관리한다. etcd는 분산 키-값 저장소로, RAFT 합의 알고리즘 기반의 리더-팔로워 구조다.

>***etcd 클러스터란***
> 
>여러 서버(etcd 노드)가 모여 하나의 일관된 Key-Value 저장소처럼 동작하며,
>합의 알고리즘으로 ‘정확한 상태 하나’를 보장하는 분산 시스템



```
[정상 상태]

┌─────────────────────────────────────────┐
│              etcd 클러스터               │
│  ┌────────┐  ┌────────┐  ┌────────┐   │
│  │노드 1  │  │노드 2  │  │노드 3  │   │
│  │(리더)  │  │(팔로워)│  │(팔로워)│   │
│  └────────┘  └────────┘  └────────┘   │
│       ↑           ↑           ↑        │
│       └───────────┼───────────┘        │
│              스위치 페어               │
└─────────────────────────────────────────┘
```

### 4.3 장애 과정

```
[타임라인]

1. 랙 스위치 중 하나에서 장애 발생

2. 노드 1이 노드 3과 통신 불가 (노드 2와는 통신 가능)
   ┌────────────────────────────────────────┐
   │  노드 1 ←→ 노드 2 ←→ 노드 3           │
   │     ↑                    ↑            │
   │     └────────✗───────────┘            │
   │         (직접 통신 불가)               │
   └────────────────────────────────────────┘

3. 노드들의 서로 다른 판단 (Byzantine-like behavior)
   - 노드 1: "노드 3이 죽었다!" → 자신에게 투표
   - 노드 2: "노드 1, 3 모두 정상" → 노드 3에게 투표
   - 노드 3: "노드 1이 죽었다!"

4. 리더 선출 결과: 타이(Tie)
   - 노드 1: 1표 (자기 자신)
   - 노드 3: 1표 (노드 2로부터)
   - 과반수 미달 → 리더 선출 실패

5. RAFT 특성: 리더 선출 중 모든 쓰기 차단

6. 반복되는 선출 시도 → 지속적인 쓰기 불가
```

### 4.4 핵심 문제: 네트워크 파티션과 일관성 없는 뷰

```
[노드별 세계관]

노드 1의 관점:           노드 2의 관점:           노드 3의 관점:
┌─────────────┐         ┌─────────────┐         ┌─────────────┐
│ 나: 정상    │         │ 나: 정상    │         │ 나: 정상    │
│ 노드2: 정상 │         │ 노드1: 정상 │         │ 노드1: ???  │
│ 노드3: 죽음 │         │ 노드3: 정상 │         │ 노드2: 정상 │
└─────────────┘         └─────────────┘         └─────────────┘

→ 각 노드가 다른 "진실"을 가지고 있음
→ 합의에 도달할 수 없음
```

> **Byzantine Failure에 가까운 현상**
>
> Cloudflare는 이 장애를 "Byzantine failure를 닮았다(resembled)"고 표현했다.
> 이론적인 Byzantine Fault 모델과 완전히 동일하지는 않지만,
> 일부 노드가 서로 다른 "현실"을 인식했다는 점에서
> 현실 세계의 Byzantine-like behavior에 가까운 사례다.

### 4.5 교훈

| 문제 | 교훈 |
|------|------|
| 부분 네트워크 파티션 | **다수의 경로**로 노드 간 통신 보장 필요 |
| 노드별 다른 뷰 | 클러스터 관리 시스템의 **파티션 감지 로직** 개선 |
| 리더 선출 중 쓰기 차단 | **읽기 전용 모드** 제공으로 영향 최소화 |

### 4.6 올바른 리더 선출 설계

```
[RAFT의 선출 타임아웃 + Jitter]

선출 타임아웃 = 기본값 + random(0, 기본값)

서버 A: 타임아웃 = 150ms + 73ms = 223ms
서버 B: 타임아웃 = 150ms + 142ms = 292ms
서버 C: 타임아웃 = 150ms + 31ms = 181ms

→ 서버 C가 먼저 선출 시도
→ 다른 서버들은 C를 리더로 인정
→ 충돌 없이 선출 완료

[하지만 Cloudflare 사례에서는...]
→ 네트워크 파티션으로 인해 노드들이 서로 다른 "현실"을 봄
→ Jitter만으로는 해결 불가
→ 네트워크 인프라 수준의 중복성이 필요
```


---

## 5. 교훈: 장애 복구의 함정

실제 사례들에서 공통적으로 나타나는 함정들이다.

### 함정 1: 장애 감지의 불확실성

| 상황 | 실제로는... |
|------|------------|
| "타임아웃 발생 = 노드 사망" | 네트워크 지연, 일시적 부하일 수 있음 |
| "하트비트 없음 = 노드 사망" | 네트워크 파티션일 수 있음 |

**GitHub 사례**: 네트워크 스위치 문제인데 노드 사망으로 오인

### 함정 2: 데이터 유실 가능성

비동기 복제 환경에서는 **데이터 유실이 불가피**하다.

```
리더:    Write A → Write B → Write C → 장애!
                              (복제 안 됨)
팔로워:  Write A → Write B → [새 리더로 승격]

결과: Write C 영구 손실
```

**GitLab 사례**: 복제 지연으로 인해 보조 서버가 한참 뒤처져 있었음

### 함정 3: 스플릿 브레인

두 노드가 모두 자신이 리더라고 믿는 상황.

**GitHub 사례**: STONITH 명령 미전달로 양쪽 노드 모두 Active

### 함정 4: 네트워크 파티션과 리더 선출 실패

**Cloudflare 사례**: 부분 네트워크 파티션 → 노드별 다른 뷰 → 리더 선출 타이 → 쓰기 전체 차단

---

## 6. 예방책

### 6.1 반동기식 복제 (Semi-Synchronous Replication)

```
[완전 비동기] - 위험
리더 → 팔로워1 (비동기) → 모든 팔로워가 뒤처질 수 있음
리더 → 팔로워2 (비동기)

[반동기식] - 권장
리더 → 팔로워1 (동기)   ← 최소 하나는 최신 상태 보장
리더 → 팔로워2 (비동기)
```

### 6.2 백업 테스트 자동화

```yaml
# GitLab이 사고 후 도입한 방식
backup_test:
  schedule: "0 3 * * *"  # 매일 새벽 3시
  steps:
    - restore_to_test_env
    - run_integrity_checks
    - compare_row_counts
    - alert_if_failed
```

### 6.3 Chaos Engineering

```
[Netflix Chaos Monkey 방식]

평소에 일부러 장애를 주입:
- 랜덤하게 노드 종료
- 네트워크 지연 주입
- 디스크 가득 참 시뮬레이션

→ 실제 장애 때 당황하지 않음
→ 장애 복구 로직의 버그를 미리 발견
```

### 6.4 Exponential Backoff + Jitter

재시도 폭풍을 방지하기 위한 필수 패턴이다.

```
[잘못된 재시도]
실패 → 즉시 재시도 → 실패 → 즉시 재시도 → ...
→ 서버에 부하 집중 → 장애 악화

[올바른 재시도: Exponential Backoff + Jitter]
실패 → 1초 후 재시도 → 2초 후 → 4초 후 → 8초 후 → ...
+ 각 간격에 무작위 지연 추가
→ 재시도 분산 → 서버 부하 완화
```

```python
import random
import time

def retry_with_backoff(func, max_retries=5, base=1, cap=32):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception:
            if attempt == max_retries - 1:
                raise
            delay = min(cap, base * (2 ** attempt))
            delay += random.uniform(0, base)  # Jitter
            time.sleep(delay)
```

### 6.5 수동 장애 복구 고려

> "일부 운영팀은 자동 장애 복구를 지원하더라도 **수동으로 장애 복구를 수행하는 것을 선호**한다."
> — DDIA 5장

| 자동 장애 복구 | 수동 장애 복구 |
|---------------|---------------|
| 빠른 복구 | 느린 복구 |
| 예기치 않은 문제 가능 | 예측 가능 |
| 새벽 3시에도 작동 | 운영자 대기 필요 |
| 외부 의존성 고려 못 함 | 체크리스트로 검증 |

#### 수동 장애 복구 체크리스트 예시

```markdown
□ 이전 리더의 상태 확인 (실제로 장애인가?)
□ 새 리더 후보의 복제 지연 확인 (몇 초/몇 분 뒤처져 있는가?)
□ 외부 시스템과의 의존성 확인 (Redis, Elasticsearch 등)
□ 이전 리더 펜싱 완료
□ 새 리더 승격
□ 클라이언트 연결 전환
□ 모니터링 알림 확인
□ 롤백 계획 준비
```

---

## 7. 정리

### 실제 사례에서 배운 것

| 사례 | 핵심 교훈 |
|------|----------|
| **GitHub 2012** | STONITH도 실패할 수 있다. 펜싱은 여러 방법 조합 필요 |
| **GitLab 2017** | 복제는 백업이 아니다. 백업은 반드시 테스트해야 한다 |
| **Cloudflare 2020** | 부분 네트워크 파티션이 리더 선출을 실패시킬 수 있다 |

### 핵심 메시지

> **자동 장애 복구를 맹신하지 마세요.**
> 장애 복구가 잘못되면 원래 장애보다 더 심각한 결과를 초래할 수 있습니다.

---

## 8. 참고 자료

### 공식 Post-Mortem

1. **GitHub Engineering Blog - Downtime Last Saturday (2012)**
   - https://github.blog/2012-12-26-downtime-last-saturday/

2. **GitLab - Postmortem of Database Outage (2017)**
   - https://about.gitlab.com/blog/postmortem-of-database-outage-of-january-31/

3. **Cloudflare Blog - A Byzantine Failure in the Real World (2020)**
   - https://blog.cloudflare.com/a-byzantine-failure-in-the-real-world/

