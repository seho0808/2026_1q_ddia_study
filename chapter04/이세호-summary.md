# Chapter 4. 인코딩과 진화 (Encoding and Evolution) (2판은 chapter 5)

> "Everything changes and nothing stands still." — Heraclitus

애플리케이션은 필연적으로 변합니다. 새로운 기능이 추가되거나 비즈니스 요구사항이 바뀌면서 데이터 구조(스키마)도 함께 변해야 합니다. 이 챕터에서는 **데이터 구조가 변할 때 시스템이 어떻게 유연하게 대처할 수 있는지(Evolvability)**, 그리고 **서로 다른 버전의 코드와 데이터가 어떻게 공존할 수 있는지**를 다룹니다.

---

## 1. 데이터와 코드의 진화 (Evolution of Data and Code)

데이터 모델의 변경은 데이터베이스 종류에 따라 다르게 처리됩니다.

- **관계형 데이터베이스 (RDBMS)**: `ALTER` 문을 통해 스키마를 명시적으로 변경합니다. 특정 시점에는 하나의 스키마만 존재합니다.
- **스키마 리스 (Schemaless / Schema-on-read)**: 스키마를 강제하지 않으므로, 데이터베이스 내에 이전 형식을 따르는 데이터와 새 형식을 따르는 데이터가 섞여 있을 수 있습니다.

애플리케이션 코드 또한 변경되는데, 대규모 시스템에서는 코드를 일시에 교체할 수 없습니다.

- **서버 측**: **무중단 배포(Rolling Upgrade)**를 위해 노드별로 순차적으로 배포하므로, 구버전 코드와 신버전 코드가 동시에 실행되는 기간이 존재합니다.
- **클라이언트 측**: 사용자가 앱 업데이트를 언제 할지 통제할 수 없으므로, 아주 오래된 버전과 최신 버전이 동시에 서버에 접근할 수 있습니다.

따라서 시스템이 원활하게 동작하려면 **양방향 호환성**이 필수적입니다.

### 1.1 하위 호환성과 상위 호환성

- **하위 호환성 (Backward Compatibility)**: **새로운 코드**가 **예전 데이터**를 읽을 수 있어야 합니다. (보통 쉬움: 예전 형식을 알고 있으므로 처리 가능)
- **상위 호환성 (Forward Compatibility)**: **예전 코드**가 **새로운 데이터**를 읽을 수 있어야 합니다. (어려움: 모르는 필드가 들어오면 무시하는 등의 처리가 필요)

### 1.2 상위 호환성의 위험: 데이터 유실

상위 호환성을 제대로 처리하지 않으면 데이터 유실이 발생할 수 있습니다.

- **상황**: 신버전 코드가 `NewField`를 추가하여 DB에 저장했습니다. 구버전 코드가 이 레코드를 읽어서 갱신한 뒤 다시 저장합니다.
- **문제**: 구버전 코드는 `NewField`를 모르기 때문에, 객체로 변환했다가 다시 인코딩하는 과정에서 **`NewField`를 누락(삭제)시킬 수 있습니다.**

---

## 2. 데이터 인코딩 형식 (Formats for Encoding Data)

프로그램은 데이터를 다룰 때 적어도 두 가지 형태를 사용합니다.

1.  **메모리 내 표현 (In-memory)**: 객체, 구조체, 배열, 해시 테이블 등. CPU가 효율적으로 접근하고 조작할 수 있도록 최적화됨 (포인터 사용).
2.  **바이트 열 표현 (Byte Sequence)**: 파일에 쓰거나 네트워크로 전송하기 위한 형태. (예: JSON 문서). 포인터는 다른 프로세스에서 의미가 없으므로 전혀 다른 형태를 띱니다.

이 두 표현 사이의 변환이 필요합니다.

- **인코딩 (Encoding / Serialization / Marshalling)**: 메모리 표현 → 바이트 열
- **디코딩 (Decoding / Parsing / Deserialization)**: 바이트 열 → 메모리 표현

> **참고**: 트랜잭션의 '직렬화(Serialization)'와 용어 충돌을 피하기 위해 이 책에서는 **인코딩**이라는 용어를 사용합니다.

---

## 3. 언어 특화 형식 (Language-Specific Formats)

많은 프로그래밍 언어가 내장 인코딩 기능을 제공합니다. (Java의 `java.io.Serializable`, Python의 `pickle`, Ruby의 `Marshal` 등)

### 장점

- 구현이 매우 간편합니다. 추가적인 코드 거의 없이 메모리 객체를 저장하고 복원할 수 있습니다.

### 단점 (치명적)

1.  **언어 종속성**: 특정 언어에 묶이게 되어, 다른 언어로 작성된 시스템과 통신하기 어렵습니다.
2.  **보안 문제**: 디코딩 과정에서 임의의 클래스를 인스턴스화할 수 있어야 하므로, 공격자가 악의적인 바이트 열을 보내 **임의 코드 실행(RCE)** 공격을 할 수 있습니다.
3.  **버전 관리 미흡**: 상위/하위 호환성을 고려하지 않고 설계된 경우가 많아, 데이터 구조 변경 시 문제가 발생하기 쉽습니다.
4.  **효율성 저하**: 인코딩/디코딩 속도가 느리고, 생성된 바이트 열의 크기도 비대할 수 있습니다. (예: Java의 기본 직렬화)

> **결론**: 일시적인 용도가 아니라면, 언어 내장 인코딩 방식은 사용하지 않는 것이 좋습니다. 대신 JSON, XML, Protocol Buffers 같은 표준화된 형식을 사용해야 합니다.

---

## 4. JSON, XML, 그리고 이진 변형 (JSON, XML, and Binary Variants)

표준화된 인코딩 방식인 JSON, XML, CSV는 다양한 언어에서 지원되며 널리 사용됩니다.

- **장점**: 텍스트 형식이므로 사람이 읽을 수 있습니다(Human-readable).
- **단점 (미묘한 문제들)**:
  1.  **수의 모호함 (Ambiguity of numbers)**: XML/CSV는 숫자와 문자열을 구분하지 않습니다. JSON은 구분하지만, **정수와 부동소수점 수를 구별하지 않고 정밀도(precision)도 명시하지 않습니다.**
      - 예: 2^53보다 큰 정수는 IEEE 754 배정밀도 부동소수점에서 정확하게 표현되지 않아, JavaScript 등에서 파싱할 때 값이 손상될 수 있습니다. (Twitter가 API에서 ID를 문자열로도 함께 보내는 이유)
  2.  **이진 문자열 미지원**: JSON/XML은 유니코드 문자열은 잘 지원하지만, 이진 데이터(바이트 열)를 위한 전용 타입이 없습니다. 보통 **Base64**로 인코딩하여 텍스트로 넣지만, 이는 데이터 크기를 33% 증가시킵니다.
  3.  **스키마 복잡성**: XML/JSON 스키마는 기능이 강력한 만큼 학습하고 구현하기 복잡합니다. CSV는 스키마가 아예 없어, 데이터의 의미(행/열 추가 등)를 애플리케이션에서 직접 관리해야 합니다.

그럼에도 불구하고, 데이터 교환 형식(Data Interchange Format)으로서는 '합의'가 중요하기 때문에 여전히 널리 쓰입니다.

---

## 5. JSON 스키마 (JSON Schema)

JSON 스키마는 시스템 간 데이터 교환이나 저장 시 데이터 모델을 정의하는 표준으로 자리 잡았습니다. OpenAPI, PostgreSQL(`pg_jsonschema`), MongoDB(`$jsonSchema`) 등에서 사용됩니다.

- **기능**: 기본 타입(문자열, 숫자, 배열 등) 외에 값에 대한 제약 조건(Validation)을 명시할 수 있습니다. (예: 포트 번호 1~65535)
- **콘텐츠 모델**:
  - **오픈 콘텐츠 모델 (Open Content Model)**: 스키마에 정의되지 않은 필드가 데이터에 포함되어도 허용합니다(기본값). 즉, "허용되지 않는 것"을 정의하는 방식에 가깝습니다.
  - **닫힌 콘텐츠 모델 (Closed Content Model)**: 명시적으로 정의된 필드만 허용합니다.
- **복잡성**: 조건부 로직이나 다른 스키마 참조 등 강력한 기능을 제공하지만, 이로 인해 정의가 복잡해지고 상위/하위 호환성을 유지하며 스키마를 발전시키기 어려울 수 있습니다.

---

## 6. 이진 인코딩 (Binary Encoding)

JSON과 XML은 텍스트 기반이라 공백을 제거해도 닫는 괄호, 따옴표, 필드 이름 반복 등으로 인해 용량이 큽니다. 이를 개선하기 위해 MessagePack, BSON, CBOR 같은 **JSON의 이진 인코딩(Binary Encoding)** 변형들이 등장했습니다.

### 특징 및 한계

- **데이터 타입 확장**: 정수/실수 구분이나 이진 데이터 지원 등 타입을 더 명확히 하기도 합니다.
- **공간 효율성 한계**: 하지만 대부분의 JSON 이진 변형은 **스키마가 없으므로**, 인코딩된 데이터 안에 **필드 이름(Key)을 계속 포함**해야 합니다.
  - 예: `userName`, `favoriteNumber` 같은 문자열이 레코드마다 반복해서 들어갑니다.
- **결과**: 텍스트 JSON(81바이트) -> MessagePack(66바이트). 공간 절약 효과가 극적이지 않으며, 가독성만 잃는 경우가 많습니다.

### MessagePack 예시 (동작 원리)

1.  첫 바이트 `0x83`: **객체(Object)**이며 필드가 **3개**임을 표시.
2.  `0xa8`: **문자열(String)**이며 길이가 **8바이트**임을 표시.
3.  `userName`: 8바이트 길이의 필드 이름을 아스키 코드로 저장.
4.  값 저장...

이 방식은 파싱 속도가 조금 빠를 수 있지만, 필드 이름을 생략할 수 없다는 근본적인 비효율은 그대로 남습니다.

---

## 7. Thrift와 Protocol Buffers

Google의 Protocol Buffers(protobuf)와 Facebook의 Thrift는 이진 인코딩 라이브러리입니다. 둘 다 인코딩할 데이터의 스키마를 정의해야 합니다.

### 7.1 특징

- **스키마 정의**: IDL(Interface Definition Language)을 사용하여 데이터 구조를 정의합니다.
- **코드 생성**: 정의된 스키마를 바탕으로 다양한 언어(Java, C++, Python 등)의 클래스 코드를 생성해주는 도구를 제공합니다.
- **압축 효율**: JSON이나 MessagePack보다 훨씬 더 작은 크기로 인코딩됩니다. (예시: 33바이트)
  - **필드 태그(Field Tag)**: 필드 이름 대신 **숫자(1, 2, 3...)**를 태그로 사용하여 필드를 식별합니다. 필드 이름은 인코딩된 데이터에 포함되지 않습니다.
  - **가변 길이 정수**: 숫자를 표현할 때 필요한 만큼만 바이트를 사용합니다.

### 7.2 스키마 발전 (Schema Evolution)

스키마가 변경되더라도 기존 데이터와의 호환성을 유지할 수 있어야 합니다.

- **필드 태그의 중요성**: 인코딩된 데이터는 필드 이름을 모르고 오직 태그 번호만 알기 때문에, **태그 번호는 절대 변경해서는 안 됩니다.** 필드 이름은 변경해도 무방합니다.
- **필드 추가 (호환성 유지)**:
  - 새로운 필드에는 새로운 태그 번호를 부여합니다.
  - **상위 호환성 (Old Code reads New Data)**: 예전 코드는 모르는 태그 번호를 만나면 해당 필드를 무시(skip)합니다. 데이터 타입 정보가 포함되어 있어 몇 바이트를 건너뛸지 알 수 있습니다.
  - **하위 호환성 (New Code reads Old Data)**: 새로운 코드는 예전 데이터에 해당 필드가 없으면 **기본값(Default Value)**을 채워서 읽습니다.
  - **주의**: `required` 필드를 추가하면 구버전 데이터 읽기에 실패하므로 하위 호환성이 깨집니다. 따라서 스키마 발전 시에는 주로 `optional`이나 `repeated` 필드를 사용해야 합니다.
- **필드 삭제**:
  - 추가의 역순입니다. `required` 필드는 삭제할 수 없습니다.
  - 삭제한 필드의 태그 번호는 다시 사용하면 안 됩니다. (혹시 모를 구버전 데이터와의 충돌 방지를 위해 `reserved`로 지정)

---

## 8. Avro (Apache Avro)

Avro는 Hadoop의 하위 프로젝트로 시작된 이진 인코딩 형식입니다. Protocol Buffers와는 설계 철학이 다릅니다.

### 8.1 특징

- **태그 번호 없음**: 스키마에 필드 태그 번호가 없습니다.
- **가장 작은 인코딩 크기**: 데이터 내에 필드나 타입을 식별하는 정보가 거의 없고 값들이 연결되어 있습니다. (예시: 32바이트)
- **정확한 스키마 필요**: 데이터를 읽으려면 데이터를 쓸 때 사용한 스키마와 구조가 정확히 일치해야 파싱할 수 있습니다.

### 8.2 쓰기 스키마와 읽기 스키마

Avro의 핵심은 데이터를 쓸 때 사용하는 스키마(**Writer's Schema**)와 읽을 때 사용하는 스키마(**Reader's Schema**)가 달라도 된다는 점입니다.

- **스키마 해석 (Schema Resolution)**: Avro 라이브러리는 쓰기 스키마와 읽기 스키마를 비교하여 데이터를 변환합니다.

  - 필드 순서가 달라도 이름으로 매칭합니다.
  - 읽기 스키마에 없고 쓰기 스키마에만 있는 필드는 무시합니다.
  - 읽기 스키마에만 있는 필드는 **기본값(Default Value)**으로 채웁니다.

- Avro 방식: "너랑 나랑 설계도를 대조해보자"
  - Avro는 필드 번호가 없고 데이터 값만 순서대로 나열되어 있습니다. 그래서 설계도가 조금이라도 어긋나면 데이터가 완전히 깨집니다. (예: 이름이 올 자리에 나이가 들어옴)
  - 해결책: 그래서 데이터를 읽을 때 두 개의 설계도를 동시에 펼쳐놓고 비교합니다.
  - Writer's Schema (보낸 놈의 설계도): "이 데이터는 이름-나이 순서로 적혀 있어."
  - Reader's Schema (받는 놈의 설계도): "나는 이름-성별-나이 순서로 읽고 싶어."
  - 작동 원리: Avro 라이브러리가 두 설계도를 대조해서 "아, 보낸 놈은 성별이 없구나. 그럼 이름 읽고, 성별은 기본값으로 채우고, 그다음에 나이를 읽어야지"라고 실시간으로 매핑을 해줍니다.

### 8.3 스키마 발전 규칙

- **기본값 필수**: 호환성을 유지하기 위해 필드를 추가하거나 삭제할 때는 반드시 기본값이 있어야 합니다.
  - 필드 추가: 기본값이 있으면 구버전 데이터(Writer)를 읽을 때(Reader) 그 값으로 채울 수 있습니다.
- **Union 타입**: `null`을 허용하려면 `union { null, long, string }` 처럼 명시해야 합니다.

### 8.4 쓰기 스키마 전달 방법

데이터 자체에 스키마 정보가 없으므로, 어떤 스키마로 쓰였는지 읽는 쪽에 알려줘야 합니다.

1.  **대용량 파일**: 파일의 시작 부분(헤더)에 쓰기 스키마를 한 번 포함시킵니다. (Avro 컨테이너 파일)
2.  **데이터베이스**: 각 레코드 앞에 **스키마 버전 번호**를 붙이고, 별도 데이터베이스에 스키마 버전들을 저장해둡니다.
3.  **네트워크**: 연결 설정 시 스키마 버전을 협상합니다.

### 8.5 동적 스키마 생성의 이점

Protocol Buffers는 필드 태그를 수동으로 관리해야 하지만, Avro는 태그가 없어서 **동적으로 스키마를 생성**하기 좋습니다.

- 예: RDBMS의 테이블 내용을 Avro 파일로 덤프할 때, 테이블 스키마가 바뀌어도 새로운 Avro 스키마를 자동 생성하기 쉽습니다. (컬럼 이름 = 필드 이름)
- 반면 Protocol Buffers는 스키마 변경 시마다 태그 번호를 신경 써서 매핑해야 하므로 자동화가 까다롭습니다.

---

## 9. 스키마의 장점 (The Merits of Schemas)

Protocol Buffers와 Avro는 스키마를 사용하여 이진 인코딩 형식을 정의합니다. 이들의 스키마 언어는 XML Schema나 JSON Schema(복잡한 유효성 검사 규칙 지원)에 비해 훨씬 단순하며, 덕분에 구현이 용이하고 다양한 프로그래밍 언어로 확산될 수 있었습니다.

### 9.1 역사적 맥락과 유사 사례 (protobuf와 avro는이미 예전부터 있던 방식임.)

- **ASN.1**: 1984년에 표준화된 스키마 정의 언어입니다. SSL 인증서(X.509)의 인코딩(DER) 등에 여전히 사용되며, Protocol Buffers처럼 태그 번호를 사용해 스키마 진화를 지원합니다. 하지만 너무 복잡하고 문서화가 부실하여 신규 프로젝트에는 적합하지 않습니다.
- **독자적 이진 인코딩**: 대부분의 RDBMS는 쿼리와 응답을 전송하기 위해 데이터베이스 고유의 이진 프로토콜을 사용합니다. (ODBC, JDBC 드라이버가 이를 인메모리 구조로 디코딩)

### 9.2 스키마 기반 이진 인코딩의 핵심 장점

JSON, XML, CSV 같은 텍스트 형식도 널리 쓰이지만, 스키마 기반의 이진 인코딩은 다음과 같은 뚜렷한 장점이 있습니다.

1.  **압축성 (Compactness)**: 인코딩된 데이터에 필드 이름을 포함할 필요가 없으므로, MessagePack 같은 '이진 JSON' 변형보다 데이터 크기가 훨씬 작습니다.
2.  **신뢰할 수 있는 문서화 (Documentation)**: 스키마는 디코딩을 위해 필수적으로 존재해야 하므로, 수동으로 관리되는 문서와 달리 실제 데이터와 절대 어긋나지 않습니다. (항상 최신 상태)
3.  **호환성 체크 (Compatibility Check)**: 스키마 레지스트리(데이터베이스)를 유지하면, 배포 전에 스키마 변경이 상위/하위 호환성을 위반하지 않는지 미리 확인할 수 있습니다.
4.  **코드 생성 (Code Generation)**: 정적 타입 언어 사용자에게 유용합니다. 스키마로부터 코드를 자동 생성하면 컴파일 시점에 타입 검사를 할 수 있어 안정성이 높아집니다.

### 9.3 요약

- JSON은 schemaless, schema-on-read임.
- JSON의 유연성을 해치지 않으면서도 스키마를 통해 데이터 형식을 보장하는 것이 있다!!
  - 바로바로 protobuf와 avro!
  - schema-on-write이다!
- schema-on-write 통신 프로토콜/도구들이 서포트를 많이 해주지만 그래도 너무 자주 스키마를 바꾸면 해롭다!

---

## 10. 데이터 흐름의 모드 (Modes of Dataflow)

메모리를 공유하지 않는 다른 프로세스로 데이터를 보낼 때(네트워크 전송, 파일 쓰기 등)는 바이트 열로 인코딩해야 합니다. 앞서 다양한 인코딩 방식을 다루었으며, 시스템의 **발전성(Evolvability)**을 위해 하위 호환성과 상위 호환성이 중요함을 이야기했습니다.

데이터가 프로세스 간에 흐르는 일반적인 방식은 다음과 같습니다.

1.  데이터베이스를 통해 (Via databases)
2.  서비스 호출을 통해 (Via service calls: REST and RPC)
3.  워크플로 엔진을 통해 (Via workflow engines)
4.  비동기 메시지를 통해 (Via asynchronous messages)

### 10.1 데이터베이스를 통한 데이터 흐름 (Dataflow Through Databases)

데이터베이스에 데이터를 저장하고 읽는 과정을 **"미래의 나에게 보내는 메시지"**라고 생각해보세요.
오늘 내가 저장한(인코딩한) 데이터를 내일의 내가 읽어서(디코딩) 사용해야 합니다.
따라서 **하위 호환성(Backward Compatibility)**은 필수입니다. 내가 예전에 써둔 일기를 오늘 읽지 못하면 안 되니까요.

하지만 실제 운영 환경에서는 상황이 좀 더 복잡합니다.

#### 1. 코드는 변해도 데이터는 남는다 (Data Outlives Code)

애플리케이션 코드는 배포를 통해 몇 분 만에 새로운 버전으로 완전히 교체될 수 있습니다.
하지만 데이터베이스 안에는 **5분 전에 저장된 데이터**와 **5년 전에 저장된 데이터**가 함께 섞여 있습니다.

- **데이터 마이그레이션의 어려움**: 스키마가 바뀔 때마다 수백 테라바이트의 기존 데이터를 모두 새 형식으로 변환(Rewrite)하는 것은 비용이 너무 크고 시스템에 부담을 줍니다.
- **해결책 (Schema Evolution)**:
  - 대부분의 데이터베이스는 예전 데이터를 굳이 건드리지 않습니다.
  - 대신 **읽을 때 해결**합니다. 예를 들어, 예전 데이터에 없는 새로운 컬럼을 조회하면 데이터베이스가 자동으로 `null`을 채워서 반환합니다.
  - 즉, **물리적인 데이터는 옛날 형식일지라도, 애플리케이션은 마치 모든 데이터가 최신 스키마인 것처럼** 읽을 수 있게 해줍니다.

#### 2. 롤링 업그레이드와 호환성

서비스를 업데이트할 때, 전 세계 모든 서버를 동시에 껐다 켤 수는 없습니다. 보통 한 대씩 순차적으로 업데이트하는 **무중단 배포(Rolling Upgrade)**를 합니다.
이 과정에서는 **구버전 코드(Old Code)**와 **신버전 코드(New Code)**가 동시에 실행되며, 같은 데이터베이스를 바라봅니다.

- **상위 호환성(Forward Compatibility)이 필요한 이유**:
  - 신버전 코드가 새로운 필드를 추가해서 DB에 저장할 수 있습니다.
  - 아직 업데이트되지 않은 **구버전 코드**가 이 데이터를 읽을 수도 있습니다.
  - 이때 구버전 코드가 "어? 이건 모르는 필드네?" 하고 에러를 내지 않고, 자연스럽게 무시할 수 있어야 합니다.

#### 3. 아카이브 저장소 (Archival storage)

백업을 하거나 데이터 분석(Data Warehouse)을 위해 데이터베이스를 통째로 복사(Dump)할 때가 있습니다.
원본 DB에는 5년 전 데이터와 최신 데이터가 섞여 있지만, **복사본을 만들 때는 최신 스키마로 통일해서 저장**하는 것이 좋습니다.
어차피 복사본은 한 번 쓰면 변하지 않는(Immutable) 데이터이므로, Avro나 Parquet 같은 분석에 최적화된 형식을 사용하여 깔끔하게 정리해두는 것이 유리합니다.

### 10.2 서비스를 통한 데이터 흐름: REST와 RPC (Dataflow Through Services: REST and RPC)

네트워크를 통해 통신해야 하는 프로세스가 있을 때, 가장 일반적인 방식은 **클라이언트와 서버** 구조입니다. 서버는 네트워크를 통해 **서비스(API)**를 노출하고, 클라이언트는 이 API로 요청을 보냅니다.

#### 웹 서비스 (Web Services)

HTTP를 기본 프로토콜로 사용하여 서비스와 통신하는 방식을 **웹 서비스**라고 합니다. 이는 웹뿐만 아니라 다양한 맥락에서 사용됩니다.

1.  **클라이언트 애플리케이션**: 모바일 앱이나 웹 브라우저의 JS 앱이 인터넷을 통해 서비스에 요청 (주로 JSON).
2.  **마이크로서비스 (Microservices)**: 같은 조직 내의 한 서비스가 다른 서비스에 요청. (서비스 지향 아키텍처, SOA)
3.  **조직 간 데이터 교환**: 다른 조직의 서비스에 요청 (예: 결제 게이트웨이, OAuth 등).

#### REST와 RPC

API 설계 철학은 크게 REST와 RPC로 나뉩니다.

- **REST (Representational State Transfer)**:

  - HTTP의 원칙을 따르는 설계 철학입니다.
  - 리소스 식별을 위해 URL을 사용하고, 캐시 제어, 인증, 콘텐츠 타입 협상 등에 HTTP 기능을 적극 활용합니다.
  - 간단한 데이터 형식을 선호하며, 인터페이스의 통일성을 강조합니다.

- **RPC (Remote Procedure Call)**:
  - **위치 투명성(Location Transparency)**: 원격 네트워크 서비스 요청을 마치 로컬 함수를 호출하는 것처럼 보이게 하려는 시도입니다. (EJB, RMI, DCOM, CORBA, SOAP 등)
  - **문제점 (근본적인 결함)**:
    - **예측 불가능성**: 로컬 호출은 성공/실패가 명확하지만, 네트워크 요청은 타임아웃 등으로 인해 요청이 도달했는지, 응답이 유실되었는지 알 수 없는 상태가 발생합니다.
    - **재시도와 멱등성**: 실패 시 재시도를 해야 하는데, 이전 요청이 실제로는 처리되었다면 중복 발생 위험이 있습니다. (멱등성 보장 필요)
    - **지연 시간 (Latency)**: 네트워크 상황에 따라 응답 속도가 매우 가변적입니다.
    - **참조 전달 불가**: 메모리 주소(포인터)를 전달할 수 없으므로, 모든 매개변수를 바이트 열로 인코딩해야 합니다. (큰 객체 전달 시 비효율)
    - **언어 간 불일치**: 클라이언트와 서버의 언어가 다르면 데이터 타입 변환이 매끄럽지 않을 수 있습니다.

> **결론**: RPC는 로컬 함수 호출과 근본적으로 다르므로, 이를 억지로 감추기보다 네트워크 통신임을 명확히 인지하고 처리하는 것이 낫습니다. REST가 인기를 얻은 이유 중 하나입니다.

#### 서비스 정의와 진화 (Service Definition and Evolution)

서비스 아키텍처(MSA)의 핵심 목표는 **서비스의 독립적인 배포와 변경(Evolvability)**을 가능하게 하는 것입니다.

- **IDL (Interface Definition Language)**:

  - 서버와 클라이언트가 통신 규약(API)을 명확히 하기 위해 사용합니다.
  - **OpenAPI (Swagger)**: REST/JSON 기반 서비스용.
  - **gRPC (Protobuf)**: Protobuf 기반의 RPC 프레임워크용.
  - IDL을 통해 코드(스텁) 생성, 문서화, 유효성 검사 등을 자동화할 수 있습니다.

- **호환성 유지 방향**:

  - 일반적으로 **서버를 먼저 업데이트**하고 클라이언트를 나중에 업데이트합니다.
  - **요청 (Requests)**: **하위 호환성** 필요 (구버전 클라이언트가 신버전 서버에 요청).
  - **응답 (Responses)**: **상위 호환성** 필요 (구버전 클라이언트가 신버전 서버의 응답(모르는 필드 포함)을 처리).

- **버전 관리**:
  - RESTful API에서는 URL 버전(`v1`), Accept 헤더 등을 통해 버전을 명시합니다.
  - API 키를 사용하는 서비스는 서버에 클라이언트별 버전을 저장하기도 합니다.

#### 서비스 디스커버리와 로드 밸런싱

서비스 인스턴스가 여러 대일 때 클라이언트가 어디로 연결해야 할지 찾는 문제입니다.

- **로드 밸런서 (L4/L7)**: 하드웨어 또는 소프트웨어(Nginx, HAProxy)가 트래픽을 분산합니다.
- **DNS**: 도메인 이름으로 IP 목록을 제공하지만, 실시간 반영이 느립니다.
- **서비스 디스커버리 (Service Discovery)**: ZooKeeper, etcd 등을 이용해 살아있는 서비스 인스턴스 목록을 관리하고 클라이언트에게 제공합니다.
- **서비스 메시 (Service Mesh)**: 사이드카 패턴(Istio, Linkerd)을 이용해 애플리케이션 코드 외부에서 정교한 라우팅, 로드 밸런싱, 관측(Observability)을 처리합니다.

### 10.3 워크플로 엔진을 통한 데이터 흐름 (Dataflow Through Workflow Engines)

서비스 기반 아키텍처(MSA)에서는 하나의 작업(예: 결제 처리)을 수행하기 위해 여러 서비스(사기 탐지, 카드 승인, 은행 입금 등)를 거쳐야 할 수 있습니다. 이러한 일련의 작업 단계를 **워크플로(Workflow)**라고 합니다.

#### 워크플로 엔진 (Workflow Engines)

워크플로 엔진은 작업을 언제 실행할지, 어떤 장비에서 실행할지, 실패 시 어떻게 할지 등을 관리합니다. 보통 **오케스트레이터(Orchestrator, 스케줄링 담당)**와 **익스큐터(Executor, 실행 담당)**로 구성됩니다.

- **Airflow, Dagster**: 데이터 시스템 통합 및 ETL 오케스트레이션에 주로 사용됩니다.
- **Camunda, Orkes**: BPMN 같은 그래픽 표기법을 사용하여 비엔지니어도 워크플로를 정의할 수 있게 돕습니다.
- **Temporal, Restate**: **내구성 있는 실행(Durable Execution)**을 제공합니다.

#### 내구성 있는 실행 (Durable Execution)

트랜잭션이 필요한 서비스 간 작업을 처리할 때 유용합니다 (예: 결제는 성공했는데 입금은 실패하면 안 됨). 분산 환경에서는 여러 서비스에 걸친 DB 트랜잭션을 묶기 어렵기 때문입니다.

- **작동 원리**: 프레임워크가 모든 RPC 호출과 상태 변경을 **내구성 있는 저장소(Write-Ahead Log 등)에 기록**합니다.
- **장애 복구**: 작업 중 실패하여 재실행될 때, **이미 성공한 단계는 건너뛰고(Skip)** 로그에 저장된 이전 결과를 반환하여 **정확히 한 번(Exactly-once)** 실행되는 것처럼 보이게 합니다.
- **제약 사항**:
  - 외부 서비스 API는 **멱등성(Idempotency)**을 보장해야 합니다 (중복 호출 방지).
  - 워크플로 코드는 **결정론적(Deterministic)**이어야 합니다. 재실행 시 동일한 순서로 동일한 명령을 내려야 하므로, 무작위 값 생성이나 시스템 시간 사용 등에 주의해야 합니다.

### 10.4 비동기 메시지를 통한 데이터 흐름 (Dataflow Through Asynchronous Messages)

데이터를 보내는 쪽(Sender)이 받는 쪽(Recipient)의 응답을 기다리지 않는 **비동기 통신** 방식입니다. 주로 **메시지 브로커(Message Broker)**라는 중개자를 거쳐 메시지(이벤트)를 전달합니다.

#### 메시지 브로커 (Message Brokers)

RabbitMQ, ActiveMQ, Apache Kafka, Google Cloud Pub/Sub 등이 있습니다.

- **장점 (RPC 대비)**:

  - **버퍼링**: 수신자가 과부하 상태일 때 메시지를 보관하여 시스템 안정성을 높입니다.
  - **신뢰성**: 수신자가 죽어도 메시지를 다시 전달할 수 있어 데이터 유실을 방지합니다.
  - **디커플링(Decoupling)**: 송신자는 누가 수신하는지 알 필요가 없습니다.
  - **일 대 다 전송**: 하나의 메시지를 여러 수신자에게 보낼 수 있습니다.

- **전송 패턴**:

  - **큐(Queue)**: 로드 밸런싱 목적. 메시지는 여러 소비자 중 하나에게만 전달됩니다.
  - **토픽(Topic, Pub/Sub)**: 구독/발행 모델. 메시지는 해당 토픽을 구독한 모든 소비자에게 전달됩니다.

- **인코딩**: 메시지 브로커는 데이터 모델을 강제하지 않고 바이트 열만 취급합니다. 따라서 JSON, Avro, Protobuf 등을 자유롭게 사용할 수 있으며, 호환성을 위해 **스키마 레지스트리**를 함께 사용하는 것이 일반적입니다.

#### 분산 액터 프레임워크 (Distributed Actor Frameworks)

Akka, Orleans, Erlang/OTP 등이 있습니다.

- **액터 모델**: 스레드 대신 **액터(Actor)** 단위로 로직을 캡슐화하고, 액터 간에는 비동기 메시지로만 통신합니다. (공유 메모리 없음, 락 불필요)
- **분산 확장**: 같은 노드에 있든 다른 노드에 있든 동일한 메시지 전달 방식을 사용하므로 **위치 투명성(Location Transparency)**이 뛰어납니다.
- **특징**: 메시지 유실 가능성을 전제로 설계되어 있어, RPC보다 분산 환경의 현실(네트워크 불안정 등)을 더 잘 반영합니다. 하지만 롤링 업그레이드 시에는 여전히 메시지의 상위/하위 호환성을 고려해야 합니다.

---

## 11. 정리 (Summary)

이 챕터에서는 데이터 구조를 네트워크나 디스크상의 바이트 열로 변환하는 다양한 방법(인코딩)을 살펴보았습니다. 인코딩의 세부 사항은 효율성뿐만 아니라 **애플리케이션의 아키텍처와 진화 가능성(Evolvability)**에 깊은 영향을 미칩니다.

### 11.1 롤링 업그레이드와 호환성

많은 서비스는 **무중단 배포(Rolling Upgrade)**를 지원해야 합니다.

- **이점**: 다운타임 없는 배포, 잦은 소규모 릴리스 장려, 배포 위험 감소(문제 발생 시 소수 사용자에게만 영향).
- **요구사항**: 롤링 업그레이드 중에는 여러 노드에서 서로 다른 버전의 코드가 동시에 실행되므로, 시스템 내의 모든 데이터 흐름은 **양방향 호환성**을 보장해야 합니다.
  - **하위 호환성 (Backward Compatibility)**: 새로운 코드가 예전 데이터를 읽을 수 있음.
  - **상위 호환성 (Forward Compatibility)**: 예전 코드가 새로운 데이터를 읽을 수 있음.

### 11.2 데이터 인코딩 형식 비교

우리는 세 가지 주요 인코딩 형식을 비교했습니다.

1.  **언어 특화 형식**: 특정 언어에 종속적이며, 상위/하위 호환성을 제공하지 못하는 경우가 많아 사용을 지양해야 합니다.
2.  **텍스트 형식 (JSON, XML, CSV)**: 널리 사용되며 호환성은 사용 방식에 따라 달라집니다. 스키마 언어가 도움이 될 수도, 방해가 될 수도 있습니다. 데이터 타입(숫자, 이진 문자열 등)의 모호함에 주의해야 합니다.
3.  **이진 스키마 기반 형식 (Protocol Buffers, Avro, Thrift)**: 작고 효율적인 인코딩을 제공하며, 상위/하위 호환성 규칙이 명확하게 정의되어 있습니다. 정적 타입 언어에서 코드 생성과 문서화에 유용하지만, 사람이 읽으려면 디코딩 도구가 필요합니다.

### 11.3 데이터 흐름의 모드

데이터 인코딩이 중요한 세 가지 주요 데이터 흐름 시나리오를 논의했습니다.

1.  **데이터베이스**: 데이터를 쓰는 프로세스가 인코딩하고, 읽는 프로세스가 디코딩합니다. (미래의 나에게 보내는 메시지)
2.  **RPC 및 REST API**: 클라이언트가 요청을 인코딩하고 서버가 디코딩하며, 서버가 응답을 인코딩하고 클라이언트가 디코딩합니다.
3.  **비동기 메시지 전달 (메시지 브로커, 액터)**: 송신자가 메시지를 인코딩하고 수신자가 디코딩합니다.

결론적으로, 약간의 주의를 기울이면 상위/하위 호환성과 롤링 업그레이드는 충분히 달성할 수 있으며, 이를 통해 애플리케이션을 빠르고 빈번하게 발전시킬 수 있습니다.
