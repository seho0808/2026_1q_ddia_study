## 04장. 부호화와 발전 (Encoding & Evolution)

## 1️⃣ 왜 ‘부호화와 발전’이 중요한가?

애플리케이션은 변하고, 데이터는 그보다 오래 산다.

기능 변경하게 된다면 저장 데이터 구조 변경은 피할 수 없다.

하지만 기존 데이터는 그대로 남아 있다.

그래서 새 코드 ↔ 옛 데이터, 옛 코드 ↔ 새 데이터 문제 발생

이 문제를 해결하는 것이 부호화(Encoding) 와 스키마 발전(Schema Evolution)

## 2️⃣ 호환성 개념
### 하위 호환성 (Backward Compatibility)

- 새 코드가 옛 데이터 읽기 가능

```
2023년에 저장한 데이터
→ 2026년에 만든 서비스 코드가 읽을 수 있어야 함
```

### 상위 호환성 (Forward Compatibility)

- 옛 코드가 새 데이터 읽기 가능
```
신규 서버 일부 배포 중
→ 옛 서버가 새 서버가 쓴 데이터를 읽음
```

### 📌 실무 포인트 

- 롤링 배포 중에는 두 호환성 모두 필요
- “모두 한 번에 배포”는 현실적으로 어려움

## 3️⃣ 데이터는 어떻게 표현되는가?
### 프로그램 내부 (In-Memory)
- 객체, 클래스, 리스트, 맵
- 포인터 기반 → 빠르지만 프로세스 내부 전용

### 저장·전송 시
- 파일 / DB / 네트워크
- 바이트 배열(byte stream) 로 변환 필요
- 이 변환이 바로 **부호화(Serialization)**

## 4️⃣ 언어 내장 직렬화의 함정 
**예: Java Serializable**

```
class User implements Serializable {
    String name;
    int age;
}
```

### 문제점
- Java 전용 → 다른 언어에서 읽기 어려움
- 보안 취약점 (역직렬화 공격)
- 스키마 변경에 취약
- 성능/크기 비효율

📌 결론

“디버깅용·임시 목적이 아니라면 사용하지 않는다”

## 5️⃣ 텍스트 기반 포맷: JSON / XML / CSV

### 장점

- 언어 독립적
- 사람이 읽을 수 있음
- 디버깅 쉬움
- REST API의 사실상 표준 (JSON)

### 단점

- 데이터 크기 큼
- 타입 정보 약함
- 필드 이름 반복 → 비효율

📌 Spring 관점

- 외부 공개 API → JSON이 좋다
- 내부 고성능 통신 → 고민 필요

## 6️⃣ 이진 부호화 (Binary Encoding)

### 왜 필요한가?

- 네트워크 트래픽 ↓
- 디스크 저장 공간 ↓
- CPU 캐시 친화적

### 예시
- MessagePack (Binary JSON)
- 같은 JSON이라도 크기 감소

📌 하지만 여전히 필드 이름 포함하며 스키마 발전에는 한계

## 7️⃣ 스리프트 & 프로토콜 버퍼 (Protobuf)

### 공통 특징

- 스키마 기반: IDL(Interface Definition Language) 사용
- 코드 자동 생성

```
message User {
  string name = 1;
  int32 age = 2;
}
```

### 핵심 개념: 필드 태그 번호

- 데이터는 name이 아니라 숫자 1, 2, 3으로 저장
- 필드 이름 변경 가능
- 태그 번호는 절대 변경 불가

## 8️⃣ 스키마 발전 (Schema Evolution)

### 필드 추가

- 새 태그 번호 사용
- 옛 코드는 모르는 필드 무시 → 상위 호환

### 필드 삭제

- optional 필드만 삭제 가능
- required 필드는 삭제 불가능

### 타입 변경

- 일부 가능 (예: int32 → int64)
- 값 잘림 위험

repeated 필드의 마법 (Protobuf)

optional → repeated 변경 가능

옛 코드는 마지막 값만 인식

## 9️⃣ Avro – 데이터 중심 시스템의 강자

### Avro의 차별점

- 태그 번호 없음

- 쓰기 스키마 + 읽기 스키마

- 가장 작은 바이너리 크기

- 스키마 자체가 데이터와 분리됨


### 읽기 과정

```
쓰기 스키마 + 읽기 스키마
→ Avro가 자동 변환
```

### 규칙

- 필드 추가/삭제 시 기본값 필수
- 필드 순서 변경 가능
- 필드 이름 변경 → alias로 대응

📌 하둡·카프카·데이터 레이크에서 인기 이유

## 🔟 데이터는 코드보다 오래 산다

- Data outlives code
- 서비스 코드는 몇 달
- 데이터는 수년~수십 년
- 대규모 데이터 마이그레이션은 비싸다

### 📌 실무 전략

“마이그레이션 없이 진화 가능한 스키마”

Avro / Protobuf 같은 설계가 중요

## 1️⃣1️⃣ 데이터플로 (Dataflow) 3가지
1. 데이터베이스

- 미래의 나에게 메시지 보내기
- 상·하위 호환 모두 중요
- ORM 재저장 시 알 수 없는 필드 유실 위험

2. 서비스(API)

- REST / RPC
- 클라이언트 제어 불가 → 호환성 장기 유지
- REST: JSON + 관용적 진화
- RPC: 성능 ↑, 복잡성 ↑

3. 메시지 브로커

- Kafka, RabbitMQ
- 비동기, 느슨한 결합
- 장애 내성
- 이벤트 기반 아키텍처 핵심

### 1️⃣2️⃣ REST vs RPC
구분	| REST	| RPC
철학	| 리소스 중심	| 함수 호출
네트워크 |  인식	명확	| 숨김
디버깅	| 쉬움	| 어려움
성능	| 보통	| 높음
내부 통신	| △	 | ◎

### 📌 실무 조합

- 외부 API → REST
- 내부 마이크로서비스 → gRPC

## 🎯 발표 마무리 핵심 요약

1. 부호화는 단순 직렬화 문제가 아니다
2. 스키마 발전은 시스템 생존 전략
3. 데이터는 코드보다 오래 산다
4. JSON은 시작점, Protobuf/Avro는 확장
5. 데이터플로에 따라 최적의 부호화 전략이 다르다